{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b0aab9",
   "metadata": {},
   "source": [
    "Aim: Separate Class into Multiple Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166397e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40ad4a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc83fa80",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('C:\\\\Users\\\\Eesh Gupta\\\\Documents\\\\RU Research\\\\Chakram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80793505",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# note: timestamp can't use \"/\" character for h5 saving.\n",
    "TIMESTAMP_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "END_OPT_STRING = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # supress warnings\n",
    "import h5py\n",
    "\n",
    "\n",
    "import ECD_control.ECD_optimization.tf_quantum as tfq\n",
    "from ECD_control.ECD_optimization.visualization import VisualizationMixin\n",
    "import qutip as qt\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be114a9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Initial Testing Code\n",
    "Setting initial and target states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a9c9e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#The target oscillator state.\n",
    "N1 =10\n",
    "N2 =10\n",
    "Fock1 = 0\n",
    "Fock2= 0\n",
    "psi_i1 = qt.basis(N1,Fock1) #target state\n",
    "psi_i2 = qt.basis(N2,Fock2)\n",
    "psi_initial = qt.tensor(psi_i1, psi_i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9b4f53",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[10, 10], [1, 1]], shape = (100, 1), type = ket\\begin{equation*}\\left(\\begin{array}{*{11}c}0.0\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\\\vdots\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[10, 10], [1, 1]], shape = (100, 1), type = ket\n",
       "Qobj data =\n",
       "[[0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fock1 = 1\n",
    "Fock2= 0\n",
    "psi_t1 = qt.basis(N1,Fock1) #target state\n",
    "psi_t2 = qt.basis(N2,Fock2)\n",
    "psi_target = qt.tensor(psi_t1, psi_t2)\n",
    "psi_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3252503c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Optimization of ECD Circuit parameters (betas, phis, and thetas)\n",
    "#the optimization options\n",
    "opt_params = {\n",
    "'N_blocks' : 5, #circuit depth\n",
    "'N_multistart' : 2, #Batch size (number of circuit optimizations to run in parallel)\n",
    "'epochs' : 100, #number of epochs before termination\n",
    "'epoch_size' : 20, #number of adam steps per epoch\n",
    "'learning_rate' : 0.01, #adam learning rate\n",
    "'term_fid' : 0.995, #terminal fidelitiy\n",
    "'dfid_stop' : 1e-6, #stop if dfid between two epochs is smaller than this number\n",
    "'beta_scale' : 3.0, #maximum |beta| for random initialization\n",
    "'gamma_scale' : 3.0, #maximum |gamma| for random initialization\n",
    "'N_cav1': N1, #number of levels in mode 1\n",
    "'N_cav2': N2, #number of levels in mode 2\n",
    "'initial_states' : [qt.tensor(qt.basis(2,0),psi_initial)], #qubit tensor oscillator, start in |g> |0>\n",
    "'target_states' : [qt.tensor(qt.basis(2,0), psi_target)], #end in |e> |target>.\n",
    "'name' : 'Fock1 %d' % Fock1, #name for printing and saving\n",
    "'filename' : None, #if no filename specified, results will be saved in this folder under 'name.h5'\n",
    "}\n",
    "\n",
    "\n",
    "#note: optimizer includes pi pulse in every ECD step. However, final ECD step is implemented \n",
    "#in experiment as a displacement since the qubit and oscillator should be disentangled at this point.\n",
    "#So, we ask the optimizer to end in |e> |target> instead of |g>|target>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1692f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a27af7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Unecessary Functions\n",
    "Functions of Batch Optimizer that don't need to be touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f3b29",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937c9cee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BatchOptimizer(VisualizationMixin):\n",
    "\n",
    "    # a block is defined as the unitary: CD(beta)D(alpha)R_phi(theta)\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimization_type=\"state transfer\",\n",
    "        target_unitary=None,\n",
    "        P_cav1=None,\n",
    "        P_cav2 = None, #EG\n",
    "        N_cav1=None,\n",
    "        N_cav2 = None, #EG\n",
    "        initial_states=None,\n",
    "        target_states=None,\n",
    "        expectation_operators=None,\n",
    "        target_expectation_values=None,\n",
    "        N_multistart=10,\n",
    "        N_blocks=20,\n",
    "        term_fid=0.99,  # can set >1 to force run all epochs\n",
    "        dfid_stop=1e-4,  # can be set= -1 to force run all epochs\n",
    "        learning_rate=0.01,\n",
    "        epoch_size=10,\n",
    "        epochs=100,\n",
    "        beta_scale=1.0,\n",
    "        gamma_scale = 1.0, #EG\n",
    "        alpha1_scale=1.0,\n",
    "        alpha2_scale=1.0,\n",
    "        theta_scale=np.pi,\n",
    "        use_etas=False,\n",
    "        use_displacements=False,\n",
    "        no_CD_end=False,\n",
    "        beta_mask=None,\n",
    "        gamma_mask = None, \n",
    "        phi_mask=None,\n",
    "        eta_mask=None,\n",
    "        theta_mask=None,\n",
    "        alpha1_mask=None,\n",
    "        alpha2_mask=None,\n",
    "        name=\"ECD_control\",\n",
    "        filename=None,\n",
    "        comment=\"\",\n",
    "        use_phase=False,  # include the phase in the optimization cost function. Important for unitaries.\n",
    "        timestamps=[],\n",
    "        **kwargs\n",
    "    ):\n",
    "        '''\n",
    "        Calls the following functions:\n",
    "        \n",
    "        construct needed matrices called\n",
    "        construct opt masks called\n",
    "        batch state transfer fids real part called\n",
    "        construct block ops batch called\n",
    "        construct displacement ops batch called\n",
    "        construct displacement ops batch called\n",
    "        '''\n",
    "        self.parameters = {\n",
    "            \"optimization_type\": optimization_type,\n",
    "            \"N_multistart\": N_multistart,\n",
    "            \"N_blocks\": N_blocks,\n",
    "            \"term_fid\": term_fid,\n",
    "            \"dfid_stop\": dfid_stop,\n",
    "            \"no_CD_end\": no_CD_end,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"epoch_size\": epoch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"beta_scale\": beta_scale,\n",
    "            \"gamma_scale\": gamma_scale, #EG\n",
    "            \"alpha1_scale\": alpha1_scale,\n",
    "            \"alpha2_scale\": alpha2_scale,\n",
    "            \"theta_scale\": theta_scale,\n",
    "            \"use_etas\": use_etas,\n",
    "            \"use_displacements\": use_displacements,\n",
    "            \"use_phase\": use_phase,\n",
    "            \"name\": name,\n",
    "            \"comment\": comment,\n",
    "        }\n",
    "        self.parameters.update(kwargs)\n",
    "        if (\n",
    "            self.parameters[\"optimization_type\"] == \"state transfer\"\n",
    "            or self.parameters[\"optimization_type\"] == \"analysis\"\n",
    "        ):\n",
    "            self.batch_fidelities = (\n",
    "                self.batch_state_transfer_fidelities\n",
    "                if self.parameters[\"use_phase\"]\n",
    "                else self.batch_state_transfer_fidelities_real_part\n",
    "            )\n",
    "            # set fidelity function\n",
    "\n",
    "            self.initial_states = tf.stack(\n",
    "                [tfq.qt2tf(state) for state in initial_states]\n",
    "            )\n",
    "\n",
    "            self.target_unitary = tfq.qt2tf(target_unitary)\n",
    "\n",
    "            # if self.target_unitary is not None: TODO\n",
    "            #     raise Exception(\"Need to fix target_unitary multi-state transfer generation!\")\n",
    "\n",
    "            self.target_states = (  # store dag\n",
    "                tf.stack([tfq.qt2tf(state) for state in target_states])\n",
    "                if self.target_unitary is None\n",
    "                else self.target_unitary @ self.initial_states\n",
    "            )\n",
    "\n",
    "            self.target_states_dag = tf.linalg.adjoint(\n",
    "                self.target_states\n",
    "            )  # store dag to avoid having to take adjoint\n",
    "\n",
    "            #N_cav1 = self.initial_states[0].numpy().shape[0] // 2\n",
    "#         elif self.parameters[\"optimization_type\"] == \"unitary\":\n",
    "#             self.target_unitary = tfq.qt2tf(target_unitary)\n",
    "#             N_cav1 = self.target_unitary.numpy().shape[0] // 2\n",
    "#             P_cav = P_cav if P_cav is not None else N_cav\n",
    "#             raise Exception(\"Need to implement unitary optimization\")\n",
    "\n",
    "        elif self.parameters[\"optimization_type\"] == \"expectation\":\n",
    "            raise Exception(\"Need to implement expectation optimization\")\n",
    "        elif (\n",
    "            self.parameters[\"optimization_type\"] == \"calculation\"\n",
    "        ):  # using functions but not doing opt\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"optimization_type must be one of {'state transfer', 'unitary', 'expectation', 'analysis', 'calculation'}\"\n",
    "            )\n",
    "\n",
    "        self.parameters[\"N_cav1\"] = N_cav1\n",
    "        self.parameters[\"N_cav2\"] = N_cav2\n",
    "#         if P_cav is not None:\n",
    "#             self.parameters[\"P_cav1\"] = P_cav1\n",
    "#             self.parameters[\"P_cav2\"] = P_cav2\n",
    "\n",
    "        # TODO: handle case when you pass initial params. In that case, don't randomize, but use \"set_tf_vars()\"\n",
    "        self.randomize_and_set_vars()\n",
    "        # self.set_tf_vars(betas=betas, alphas=alphas, phis=phis, thetas=thetas)\n",
    "\n",
    "        self._construct_needed_matrices()\n",
    "        self._construct_optimization_masks(beta_mask,gamma_mask, alpha1_mask, alpha2_mask, phi_mask,eta_mask, theta_mask)\n",
    "\n",
    "        # opt data will be a dictionary of dictonaries used to store optimization data\n",
    "        # the dictionary will be addressed by timestamps of optmization.\n",
    "        # each opt will append to opt_data a dictionary\n",
    "        # this dictionary will contain optimization parameters and results\n",
    "\n",
    "        self.timestamps = timestamps\n",
    "        self.filename = (\n",
    "            filename\n",
    "            if (filename is not None and filename != \"\")\n",
    "            else self.parameters[\"name\"]\n",
    "        )\n",
    "        path = self.filename.split(\".\")\n",
    "        if len(path) < 2 or (len(path) == 2 and path[-1] != \".h5\"):\n",
    "            self.filename = path[0] + \".h5\"\n",
    "\n",
    "#     def modify_parameters(self, **kwargs):\n",
    "#         print('modify parameters called')\n",
    "#         # currently, does not support changing optimization type.\n",
    "#         # todo: update for multi-state optimization and unitary optimziation\n",
    "#         parameters = kwargs\n",
    "#         for param, value in self.parameters.items():\n",
    "#             if param not in parameters:\n",
    "#                 parameters[param] = value\n",
    "#         # handle things that are not in self.parameters:\n",
    "#         parameters[\"initial_states\"] = (\n",
    "#             parameters[\"initial_states\"]\n",
    "#             if \"initial_states\" in parameters\n",
    "#             else self.initial_states\n",
    "#         )\n",
    "#         parameters[\"target_states\"] = (\n",
    "#             parameters[\"target_states\"]\n",
    "#             if \"target_states\" in parameters\n",
    "#             else self.target_states\n",
    "#         )\n",
    "#         parameters[\"filename\"] = (\n",
    "#             parameters[\"filename\"] if \"filename\" in parameters else self.filename\n",
    "#         )\n",
    "#         parameters[\"timestamps\"] = (\n",
    "#             parameters[\"timestamps\"] if \"timestamps\" in parameters else self.timestamps\n",
    "#         )\n",
    "#         self.__init__(**parameters)\n",
    "\n",
    "    def _construct_needed_matrices(self):\n",
    "        \"\"\"\n",
    "        EG: this function is making the a, a_dagger matrices for the cavity modes\n",
    "        and these functions will be later exponentiated to get the displacement operator\n",
    "        \n",
    "        Edited to inclide 2nd mode\n",
    "        \"\"\"\n",
    "        #print('construct needed matrices called')\n",
    "        N_cav1 = self.parameters[\"N_cav1\"]\n",
    "        N_cav2 = self.parameters[\"N_cav2\"]\n",
    "        q1 = tfq.position(N_cav1)\n",
    "        p1 = tfq.momentum(N_cav1)\n",
    "        q2 = tfq.position(N_cav2)\n",
    "        p2 = tfq.momentum(N_cav2)\n",
    "\n",
    "        # Pre-diagonalize\n",
    "        (self._eig_q1, self._U_q1) = tf.linalg.eigh(q1)\n",
    "        (self._eig_p1, self._U_p1) = tf.linalg.eigh(p1)\n",
    "        (self._eig_q2, self._U_q2) = tf.linalg.eigh(q2)\n",
    "        (self._eig_p2, self._U_p2) = tf.linalg.eigh(p2)\n",
    "\n",
    "        self._qp1_comm = tf.linalg.diag_part(q1 @ p1 - p1 @ q1)\n",
    "        self._qp2_comm = tf.linalg.diag_part(q2 @ p2 - p2 @ q2)\n",
    "        #first mode\n",
    "        if self.parameters[\"optimization_type\"] == \"unitary\":\n",
    "            P_cav1 = self.parameters[\"P_cav1\"]\n",
    "            partial_I1 = np.array(qt.identity(N_cav1))\n",
    "            for j in range(P_cav1, N_cav1):\n",
    "                partial_I1[j, j] = 0\n",
    "            partial_I1 = qt.Qobj(partial_I1)\n",
    "            self.P1_matrix = tfq.qt2tf(qt.tensor(qt.identity(2), partial_I1))\n",
    "        #second mode\n",
    "        if self.parameters[\"optimization_type\"] == \"unitary\":\n",
    "            P_cav2 = self.parameters[\"P_cav2\"]\n",
    "            partial_I2 = np.array(qt.identity(N_cav2))\n",
    "            for j in range(P_cav2, N_cav2):\n",
    "                partial_I2[j, j] = 0\n",
    "            partial_I2 = qt.Qobj(partial_I2)\n",
    "            self.P2_matrix = tfq.qt2tf(qt.tensor(qt.identity(2), partial_I2))\n",
    "\n",
    "    def _construct_optimization_masks(\n",
    "        self, beta_mask=None,gamma_mask = None, alpha1_mask=None, alpha2_mask=None, phi_mask=None,eta_mask=None, theta_mask=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        EG: What is a mask?\n",
    "        \n",
    "        Edit: Added gamma\n",
    "        \"\"\"\n",
    "        #print('construct opt masks called')\n",
    "        if beta_mask is None:\n",
    "            beta_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            if self.parameters[\"no_CD_end\"]:\n",
    "                beta_mask[-1, :] = 0  # don't optimize final CD\n",
    "        else:\n",
    "            # TODO: add mask to self.parameters for saving if it's non standard!\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if gamma_mask is None:\n",
    "            gamma_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            if self.parameters[\"no_CD_end\"]:\n",
    "                gamma_mask[-1, :] = 0  # don't optimize final CD\n",
    "        else:\n",
    "            # TODO: add mask to self.parameters for saving if it's non standard!\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if alpha1_mask is None:\n",
    "            alpha1_mask = np.ones(\n",
    "                shape=(1, self.parameters[\"N_multistart\"]), dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if alpha2_mask is None:\n",
    "            alpha2_mask = np.ones(\n",
    "                shape=(1, self.parameters[\"N_multistart\"]), dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if phi_mask is None:\n",
    "            phi_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            phi_mask[0, :] = 0  # stop gradient on first phi entry\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if eta_mask is None:\n",
    "            eta_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            phi_mask[0, :] = 0  # stop gradient on first phi entry\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if theta_mask is None:\n",
    "            theta_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        self.beta_mask = beta_mask\n",
    "        self.gamma_mask = gamma_mask\n",
    "        self.alpha1_mask = alpha1_mask\n",
    "        self.alpha2_mask = alpha2_mask\n",
    "        self.phi_mask = phi_mask\n",
    "        self.eta_mask = eta_mask\n",
    "        self.theta_mask = theta_mask\n",
    "\n",
    "    @tf.function\n",
    "    def batch_construct_displacement_operators(self, alphas1, alphas2):\n",
    "        '''\n",
    "        Input: a list of displacements for \n",
    "        '''\n",
    "        #print('construct displacement ops batch called')\n",
    "        # Reshape amplitudes for broadcast against diagonals\n",
    "        sqrt2 = tf.math.sqrt(tf.constant(2, dtype=tf.complex64))\n",
    "        \n",
    "        re_a1 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.real(alphas1), dtype=tf.complex64),\n",
    "            [alphas1.shape[0], alphas1.shape[1], 1],\n",
    "        )\n",
    "        im_a1 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.imag(alphas1), dtype=tf.complex64),\n",
    "            [alphas1.shape[0], alphas1.shape[1], 1],\n",
    "        )\n",
    "        \n",
    "        re_a2 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.real(alphas2), dtype=tf.complex64),\n",
    "            [alphas1.shape[0], alphas1.shape[1], 1],\n",
    "        )\n",
    "        im_a2 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.imag(alphas2), dtype=tf.complex64),\n",
    "            [alphas2.shape[0], alphas2.shape[1], 1],\n",
    "        )\n",
    "\n",
    "        # Exponentiate diagonal matrices\n",
    "        #first mode\n",
    "        expm_q1 = tf.linalg.diag(tf.math.exp(1j * im_a1 * self._eig_q1))\n",
    "        expm_p1 = tf.linalg.diag(tf.math.exp(-1j * re_a1 * self._eig_p1))\n",
    "        expm_c1 = tf.linalg.diag(tf.math.exp(-0.5 * re_a1 * im_a1 * self._qp1_comm))\n",
    "        #second mode\n",
    "        expm_q2 = tf.linalg.diag(tf.math.exp(1j * im_a2 * self._eig_q2))\n",
    "        expm_p2 = tf.linalg.diag(tf.math.exp(-1j * re_a2 * self._eig_p2))\n",
    "        expm_c2 = tf.linalg.diag(tf.math.exp(-0.5 * re_a2 * im_a2 * self._qp2_comm))\n",
    "\n",
    "        \n",
    "        # Apply Baker-Campbell-Hausdorff to each\n",
    "        disp1s =  tf.cast(\n",
    "            self._U_q1\n",
    "            @ expm_q1\n",
    "            @ tf.linalg.adjoint(self._U_q1)\n",
    "            @ self._U_p1\n",
    "            @ expm_p1\n",
    "            @ tf.linalg.adjoint(self._U_p1)\n",
    "            @ expm_c1,\n",
    "            dtype=tf.complex64,\n",
    "        )\n",
    "        disp2s =  tf.cast(\n",
    "            self._U_q2\n",
    "            @ expm_q2\n",
    "            @ tf.linalg.adjoint(self._U_q2)\n",
    "            @ self._U_p2\n",
    "            @ expm_p2\n",
    "            @ tf.linalg.adjoint(self._U_p2)\n",
    "            @ expm_c2,\n",
    "            dtype=tf.complex64,\n",
    "        )\n",
    "        \n",
    "        #each dispNs object has shape (num_layers ,num_multistarts, N_cav, N_cav)\n",
    "        #so each object contains num_multistatrs disp operators\n",
    "        # so we gotta kron product disp op for mode 1 and 2 for each multistart in each layer\n",
    "        num_layers = len(alphas1)\n",
    "        num_multistarts = len(alphas1[0])\n",
    "        layer_matrices= []\n",
    "        for l in range(num_layers):\n",
    "            multistart_matrices = []\n",
    "            for m in range(num_multistarts): \n",
    "                d1 = disp1s[l][m]\n",
    "                d2 = disp2s[l][m]\n",
    "                #print(d1)\n",
    "                tf.compat.v1.enable_eager_execution(\n",
    "                config=None, device_policy=None, execution_mode=None\n",
    "                ) # to make following tensor--> numpy matrix work\n",
    "                operator_1 = tf.linalg.LinearOperatorFullMatrix(d1.numpy())\n",
    "                operator_2 = tf.linalg.LinearOperatorFullMatrix(d2.numpy())\n",
    "                operator = tf.linalg.LinearOperatorKronecker([operator_1, operator_2])\n",
    "                mat = tf.cast(operator.to_dense().numpy(), dtype = tf.complex64)\n",
    "                multistart_matrices.append(mat)\n",
    "            layer_matrices.append(multistart_matrices)\n",
    "        \n",
    "        kron_disps = tf.cast(layer_matrices, dtype =tf.complex64 )\n",
    "        return kron_disps\n",
    "\n",
    "    @tf.function\n",
    "    def batch_construct_block_operators(\n",
    "        self, betas_rho, betas_angle,gammas_rho, gammas_angle,\n",
    "        alphas1_rho, alphas1_angle, alphas2_rho, alphas2_angle,\n",
    "        phis, etas, thetas\n",
    "    ):\n",
    "        #print('construct block ops batch called')\n",
    "        # conditional displacements  (EG: a list of complex #s)\n",
    "        Bs = (\n",
    "            tf.cast(betas_rho, dtype=tf.complex64)\n",
    "            / tf.constant(2, dtype=tf.complex64)\n",
    "            * tf.math.exp(\n",
    "                tf.constant(1j, dtype=tf.complex64)\n",
    "                * tf.cast(betas_angle, dtype=tf.complex64)\n",
    "            )\n",
    "        )\n",
    "        Gs = (\n",
    "            tf.cast(gammas_rho, dtype=tf.complex64)\n",
    "            / tf.constant(2, dtype=tf.complex64)\n",
    "            * tf.math.exp(\n",
    "                tf.constant(1j, dtype=tf.complex64)\n",
    "                * tf.cast(gammas_angle, dtype=tf.complex64)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # final displacement \n",
    "        #(EG: a list of complex #s, each complex num is alpha)\n",
    "        D1 = tf.cast(alphas1_rho, dtype=tf.complex64) * tf.math.exp(\n",
    "            tf.constant(1j, dtype=tf.complex64)\n",
    "            * tf.cast(alphas1_angle, dtype=tf.complex64)\n",
    "        )\n",
    "        D2 = tf.cast(alphas2_rho, dtype=tf.complex64) * tf.math.exp(\n",
    "            tf.constant(1j, dtype=tf.complex64)\n",
    "            * tf.cast(alphas2_angle, dtype=tf.complex64)\n",
    "        )\n",
    "\n",
    "        ds_end = self.batch_construct_displacement_operators(D1, D2)\n",
    "        ds_g = self.batch_construct_displacement_operators(Bs, Gs)\n",
    "        ds_e = tf.linalg.adjoint(ds_g)\n",
    "\n",
    "        # phi = phi - pi/2\n",
    "        Phis = phis - tf.constant(np.pi, dtype=tf.float32) / tf.constant(\n",
    "            2, dtype=tf.float32\n",
    "        )\n",
    "        #theta = theta/2\n",
    "        Thetas = thetas / tf.constant(2, dtype=tf.float32)\n",
    "        \n",
    "        #Reshaping these lists of angles for some reason\n",
    "        Phis = tf.cast(\n",
    "            tf.reshape(Phis, [Phis.shape[0], Phis.shape[1], 1, 1]), dtype=tf.complex64\n",
    "        )\n",
    "        etas = tf.cast(\n",
    "            tf.reshape(etas, [etas.shape[0], etas.shape[1], 1, 1]), dtype=tf.complex64\n",
    "        )\n",
    "        Thetas = tf.cast(\n",
    "            tf.reshape(Thetas, [Thetas.shape[0], Thetas.shape[1], 1, 1]),\n",
    "            dtype=tf.complex64,\n",
    "        )\n",
    "        #e^iphi\n",
    "        exp = tf.math.exp(tf.constant(1j, dtype=tf.complex64) * Phis)\n",
    "        im = tf.constant(1j, dtype=tf.complex64)\n",
    "        exp_dag = tf.linalg.adjoint(exp)\n",
    "        cos = tf.math.cos(Thetas)\n",
    "        sin = tf.math.sin(Thetas)\n",
    "        cos_e = tf.math.cos(etas)\n",
    "        sin_e = tf.math.sin(etas)\n",
    "\n",
    "        # constructing the blocks of the matrix\n",
    "        ul = (cos + im * sin * cos_e) * ds_g\n",
    "        ll = exp * sin * sin_e * ds_e\n",
    "        ur = tf.constant(-1, dtype=tf.complex64) * exp_dag * sin * sin_e * ds_g\n",
    "        lr = (cos - im * sin * cos_e) * ds_e\n",
    "\n",
    "        # without pi pulse, block matrix is:\n",
    "        # (ul, ur)\n",
    "        # (ll, lr)\n",
    "        # however, with pi pulse included:\n",
    "        # (ll, lr)\n",
    "        # (ul, ur)\n",
    "        # pi pulse also adds -i phase, however don't need to trck it unless using multiple oscillators.a\n",
    "        # append a final block matrix with a single displacement in each quadrant\n",
    "        blocks = tf.concat(\n",
    "            [\n",
    "                -1j * tf.concat([tf.concat([ll, lr], 3), tf.concat([ul, ur], 3)], 2),\n",
    "                tf.concat(\n",
    "                    [\n",
    "                        tf.concat([ds_end, tf.zeros_like(ds_end)], 3),\n",
    "                        tf.concat([tf.zeros_like(ds_end), ds_end], 3),\n",
    "                    ],\n",
    "                    2,\n",
    "                ),\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        return blocks\n",
    "\n",
    "    # batch computation of <D>\n",
    "    # todo: handle non-pure states (rho)\n",
    "#     def characteristic_function(self, psi, betas):\n",
    "#         print('characteristic_function called')\n",
    "#         psi = tfq.qt2tf(psi)\n",
    "#         betas_flat = betas.flatten()\n",
    "#         betas_tf = tf.constant(\n",
    "#             [betas_flat]\n",
    "#         )  # need to add extra dimension since it usually batches circuits\n",
    "#         Ds = tf.squeeze(self.batch_construct_displacement_operators(betas_tf))\n",
    "#         num_pts = betas_tf.shape[1]\n",
    "#         psis = tf.constant(np.array([psi] * num_pts))\n",
    "#         C = tf.linalg.adjoint(psis) @ Ds @ psis\n",
    "#         return np.squeeze(C.numpy()).reshape(betas.shape)\n",
    "\n",
    "#     def characteristic_function_rho(self, rho, betas):\n",
    "#         print('characteristic_function rho called')\n",
    "#         rho = tfq.qt2tf(rho)\n",
    "#         betas_flat = betas.flatten()\n",
    "#         betas_tf = tf.constant(\n",
    "#             [betas_flat]\n",
    "#         )  # need to add extra dimension since it usually batches circuits\n",
    "#         Ds = tf.squeeze(self.batch_construct_displacement_operators(betas_tf))\n",
    "#         num_pts = betas_tf.shape[1]\n",
    "#         rhos = tf.constant(np.array([rho] * num_pts))\n",
    "#         C = tf.linalg.trace(Ds @ rhos)\n",
    "#         return np.squeeze(C.numpy()).reshape(betas.shape)\n",
    "\n",
    "    \"\"\"\n",
    "    @tf.function\n",
    "    def state(\n",
    "        self,\n",
    "        i=0,\n",
    "        betas_rho=None,\n",
    "        betas_angle=None,\n",
    "        alphas_rho=None,\n",
    "        alphas_angle=None,\n",
    "        phis=None,\n",
    "        thetas=None,\n",
    "    ):\n",
    "        betas_rho = self.betas_rho if betas_rho is None else betas_rho\n",
    "        betas_angle = self.betas_angle if betas_angle is None else betas_angle\n",
    "        alphas_rho = self.alphas_rho if alphas_rho is None else alphas_rho\n",
    "        alphas_angle = self.alphas_angle if alphas_angle is None else alphas_angle\n",
    "        phis = self.phis if phis is None else phis\n",
    "        thetas = self.thetas if thetas is None else thetas\n",
    "        if self.parameters[\"use_displacements\"]:\n",
    "            bs = self.construct_block_operators_with_displacements(\n",
    "                betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "            )\n",
    "        else:\n",
    "            bs = self.construct_block_operators(betas_rho, betas_angle, phis, thetas)\n",
    "        psi = self.initial_states[0]\n",
    "        for U in bs[:i]:\n",
    "            psi = U @ psi\n",
    "        return psi\n",
    "    \"\"\"\n",
    "\n",
    "#     @tf.function\n",
    "#     def batch_state_transfer_fidelities(\n",
    "#         self, betas_rho, betas_angle, alphas_rho, alphas_angle, phis, etas, thetas\n",
    "#     ):\n",
    "#         print('batch state transfer fids called')\n",
    "#         bs = self.batch_construct_block_operators(\n",
    "#             betas_rho, betas_angle, alphas_rho, alphas_angle, phis, etas, thetas\n",
    "#         )\n",
    "#         psis = tf.stack([self.initial_states] * self.parameters[\"N_multistart\"])\n",
    "#         for U in bs:\n",
    "#             psis = tf.einsum(\n",
    "#                 \"mij,msjk->msik\", U, psis\n",
    "#             )  # m: multistart, s:multiple states\n",
    "#         overlaps = self.target_states_dag @ psis  # broadcasting\n",
    "#         overlaps = tf.reduce_mean(overlaps, axis=1)\n",
    "#         overlaps = tf.squeeze(overlaps)\n",
    "#         # squeeze after reduce_mean which uses axis=1,\n",
    "#         # which will not exist if squeezed before for single state transfer\n",
    "#         fids = tf.cast(overlaps * tf.math.conj(overlaps), dtype=tf.float32)\n",
    "#         return fids\n",
    "\n",
    "    # here, including the relative phase in the cost function by taking the real part of the overlap then squaring it.\n",
    "    # need to think about how this is related to the fidelity.\n",
    "    @tf.function\n",
    "    def batch_state_transfer_fidelities_real_part(\n",
    "        self, betas_rho, betas_angle,gammas_rho, gammas_angle,\n",
    "        alphas1_rho, alphas1_angle, alphas2_rho, alphas2_angle,\n",
    "        phis, etas, thetas\n",
    "    ):\n",
    "        #print('batch state transfer fids real part called')\n",
    "        bs = self.batch_construct_block_operators(\n",
    "            betas_rho, betas_angle,gammas_rho, gammas_angle,\n",
    "            alphas1_rho, alphas1_angle, alphas2_rho, alphas2_angle,\n",
    "            phis, etas, thetas\n",
    "        )\n",
    "        psis = tf.stack([self.initial_states] * self.parameters[\"N_multistart\"])\n",
    "        for U in bs:\n",
    "            psis = tf.einsum(\n",
    "                \"mij,msjk->msik\", U, psis\n",
    "            )  # m: multistart, s:multiple states\n",
    "        overlaps = self.target_states_dag @ psis  # broadcasting\n",
    "        overlaps = tf.reduce_mean(tf.math.real(overlaps), axis=1)\n",
    "        overlaps = tf.squeeze(overlaps)\n",
    "        # squeeze after reduce_mean which uses axis=1,\n",
    "        # which will not exist if squeezed before for single state transfer\n",
    "        # don't need to take the conjugate anymore\n",
    "        fids = tf.cast(overlaps * overlaps, dtype=tf.float32)\n",
    "        return fids\n",
    "\n",
    "#     @tf.function\n",
    "#     def mult_bin_tf(self, a):\n",
    "#         print('mult bin tf called')\n",
    "#         while a.shape[0] > 1:\n",
    "#             if a.shape[0] % 2 == 1:\n",
    "#                 a = tf.concat(\n",
    "#                     [a[:-2], [tf.matmul(a[-2], a[-1])]], 0\n",
    "#                 )  # maybe there's a faster way to deal with immutable constants\n",
    "#             a = tf.matmul(a[::2, ...], a[1::2, ...])\n",
    "#         return a[0]\n",
    "\n",
    "#     @tf.function\n",
    "#     def U_tot(self,):\n",
    "#         print('Utot called')\n",
    "#         bs = self.batch_construct_block_operators(\n",
    "#             self.betas_rho,\n",
    "#             self.betas_angle,\n",
    "#             self.alphas_rho,\n",
    "#             self.alphas_angle,\n",
    "#             self.phis,\n",
    "#             self.etas,\n",
    "#             self.thetas,\n",
    "#         )\n",
    "#         # U_c = tf.scan(lambda a, b: tf.matmul(b, a), bs)[-1]\n",
    "#         U_c = self.mult_bin_tf(\n",
    "#             tf.reverse(bs, axis=[0])\n",
    "#         )  # [U_1,U_2,..] -> [U_N,U_{N-1},..]-> U_N @ U_{N-1} @ .. @ U_1\n",
    "#         # U_c = self.I\n",
    "#         # for U in bs:\n",
    "#         #     U_c = U @ U_c\n",
    "#         return U_c\n",
    "\n",
    "    \"\"\"\n",
    "    @tf.function\n",
    "    def unitary_fidelity(\n",
    "        self, betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "    ):\n",
    "        U_circuit = self.U_tot(\n",
    "            betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "        )\n",
    "        D = tf.constant(self.parameters[\"P_cav\"] * 2, dtype=tf.complex64)\n",
    "        overlap = tf.linalg.trace(\n",
    "            tf.linalg.adjoint(self.target_unitary) @ self.P_matrix @ U_circuit\n",
    "        )\n",
    "        return tf.cast(\n",
    "            (1.0 / D) ** 2 * overlap * tf.math.conj(overlap), dtype=tf.float32\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"EG: Main function, most of the body is in catching er\"\"\" \n",
    "#     def optimize(self, do_prints=True):\n",
    "#         #print('optimize called')\n",
    "#         timestamp = datetime.datetime.now().strftime(TIMESTAMP_FORMAT)\n",
    "#         self.timestamps.append(timestamp)\n",
    "#         print(\"Start time: \" + timestamp)\n",
    "#         # start time\n",
    "#         start_time = time.time()\n",
    "#         optimizer = tf.optimizers.Ftrl(self.parameters[\"learning_rate\"])\n",
    "#         if self.parameters[\"use_displacements\"] and self.parameters[\"use_etas\"]:\n",
    "#             variables = [\n",
    "#                 self.betas_rho,\n",
    "#                 self.betas_angle,\n",
    "#                 self.gammas_rho,\n",
    "#                 self.gammas_angle,\n",
    "#                 self.alphas1_rho,\n",
    "#                 self.alphas1_angle,\n",
    "#                 self.alphas2_rho,\n",
    "#                 self.alphas2_angle,\n",
    "#                 self.phis,\n",
    "#                 self.etas,\n",
    "#                 self.thetas,\n",
    "#             ]\n",
    "#         elif self.parameters[\"use_etas\"]:\n",
    "#             variables = [\n",
    "#                 self.betas_rho,\n",
    "#                 self.betas_angle,\n",
    "#                 self.gammas_rho,\n",
    "#                 self.gammas_angle,\n",
    "#                 self.phis,\n",
    "#                 self.etas,\n",
    "#                 self.thetas,\n",
    "#             ]\n",
    "#         elif self.parameters[\"use_displacements\"]:\n",
    "#             variables = [\n",
    "#                 self.betas_rho,\n",
    "#                 self.betas_angle,\n",
    "#                 self.gammas_rho,\n",
    "#                 self.gammas_angle,\n",
    "#                 self.alphas1_rho,\n",
    "#                 self.alphas1_angle,\n",
    "#                 self.alphas2_rho,\n",
    "#                 self.alphas2_angle,\n",
    "#                 self.phis,\n",
    "#                 self.thetas,\n",
    "#             ]\n",
    "#         else:\n",
    "#             variables = [\n",
    "#                 self.betas_rho,\n",
    "#                 self.betas_angle,\n",
    "#                 self.gammas_rho,\n",
    "#                 self.gammas_angle,\n",
    "#                 self.phis,\n",
    "#                 self.thetas,\n",
    "#             ]\n",
    "\n",
    "#         @tf.function\n",
    "#         def entry_stop_gradients(target, mask):\n",
    "#             #print('entry stop grad called')\n",
    "#             mask_h = tf.abs(mask - 1)\n",
    "#             return tf.stop_gradient(mask_h * target) + mask * target\n",
    "\n",
    "#         \"\"\"\n",
    "#         if self.optimize_expectation:\n",
    "#             @tf.function\n",
    "#             def loss_fun(\n",
    "#                 betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#             ):\n",
    "#                 expect = self.expectation_value(\n",
    "#                     betas_rho,\n",
    "#                     betas_angle,\n",
    "#                     alphas_rho,\n",
    "#                     alphas_angle,\n",
    "#                     phis,\n",
    "#                     thetas,\n",
    "#                     self.O,\n",
    "#                 )\n",
    "#                 return tf.math.log(1 - tf.math.real(expect))\n",
    "#         if self.unitary_optimization:\n",
    "#             if self.unitary_optimization == \"states\":\n",
    "#                 @tf.function\n",
    "#                 def loss_fun(\n",
    "#                     betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#                 ):\n",
    "#                     fid = self.unitary_fidelity_state_decomp(\n",
    "#                         betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#                     )\n",
    "#                     return tf.math.log(1 - fid)\n",
    "#             else:\n",
    "#                 @tf.function\n",
    "#                 def loss_fun(\n",
    "#                     betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#                 ):\n",
    "#                     fid = self.unitary_fidelity(\n",
    "#                         betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#                     )\n",
    "#                     return tf.math.log(1 - fid)\n",
    "#         else:\n",
    "#             @tf.function\n",
    "#             def loss_fun(\n",
    "#                 betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#             ):\n",
    "#                 fid = self.state_fidelity(\n",
    "#                     betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "#                 )e\n",
    "#                 return tf.math.log(1 - fid)\n",
    "#         \"\"\"\n",
    "\n",
    "#         @tf.function\n",
    "#         def loss_fun(fids):\n",
    "#             #print('loss fun called')\n",
    "#             # I think it's important that the log is taken before the avg\n",
    "#             losses = tf.math.log(1 - fids)\n",
    "#             avg_loss = tf.reduce_sum(losses) / self.parameters[\"N_multistart\"]\n",
    "#             return avg_loss\n",
    "\n",
    "#         def callback_fun(obj, fids, dfids, epoch):\n",
    "#             #print('callback fun called')\n",
    "#             elapsed_time_s = time.time() - start_time\n",
    "#             time_per_epoch = elapsed_time_s / epoch if epoch != 0 else 0.0\n",
    "#             epochs_left = self.parameters[\"epochs\"] - epoch\n",
    "#             expected_time_remaining = epochs_left * time_per_epoch\n",
    "#             fidelities_np = np.squeeze(np.array(fids))\n",
    "#             betas_np, gammas_np, alphas1_np, alphas2_np, phis_np, etas_np, thetas_np = self.get_numpy_vars()\n",
    "#             if epoch == 0:\n",
    "#                 self._save_optimization_data(\n",
    "#                     timestamp,\n",
    "#                     fidelities_np,\n",
    "#                     betas_np,\n",
    "#                     gammas_np,\n",
    "#                     alphas1_np,\n",
    "#                     alphas2_np,\n",
    "#                     phis_np,\n",
    "#                     etas_np,\n",
    "#                     thetas_np,\n",
    "#                     elapsed_time_s,\n",
    "#                     append=False,\n",
    "#                 )\n",
    "#             else:\n",
    "#                 self._save_optimization_data(\n",
    "#                     timestamp,\n",
    "#                     fidelities_np,\n",
    "#                     betas_np,\n",
    "#                     gammas_np,\n",
    "#                     alphas1_np,\n",
    "#                     alphas2_np,\n",
    "#                     phis_np,\n",
    "#                     etas_np,\n",
    "#                     thetas_np,\n",
    "#                     elapsed_time_s,\n",
    "#                     append=True,\n",
    "#                 )\n",
    "#             avg_fid = tf.reduce_sum(fids) / self.parameters[\"N_multistart\"]\n",
    "#             max_fid = tf.reduce_max(fids)\n",
    "#             avg_dfid = tf.reduce_sum(dfids) / self.parameters[\"N_multistart\"]\n",
    "#             max_dfid = tf.reduce_max(dfids)\n",
    "#             extra_string = \" (real part)\" if self.parameters[\"use_phase\"] else \"\"\n",
    "#             if do_prints:\n",
    "#                 print(\n",
    "#                     \"\\r Epoch: %d / %d Max Fid: %.6f Avg Fid: %.6f Max dFid: %.6f Avg dFid: %.6f\"\n",
    "#                     % (\n",
    "#                         epoch,\n",
    "#                         self.parameters[\"epochs\"],\n",
    "#                         max_fid,\n",
    "#                         avg_fid,\n",
    "#                         max_dfid,\n",
    "#                         avg_dfid,\n",
    "#                     )\n",
    "#                     + \" Elapsed time: \"\n",
    "#                     + str(datetime.timedelta(seconds=elapsed_time_s))\n",
    "#                     + \" Remaing time: \"\n",
    "#                     + str(datetime.timedelta(seconds=expected_time_remaining))\n",
    "#                     + extra_string,\n",
    "#                     end=\"\",\n",
    "#                 )\n",
    "\n",
    "#         initial_fids = self.batch_fidelities(\n",
    "#             self.betas_rho,\n",
    "#             self.betas_angle,\n",
    "#             self.gammas_rho,\n",
    "#             self.gammas_angle,\n",
    "#             self.alphas1_rho,\n",
    "#             self.alphas1_angle,\n",
    "#             self.alphas2_rho,\n",
    "#             self.alphas2_angle,\n",
    "#             self.phis,\n",
    "#             self.etas,\n",
    "#             self.thetas,\n",
    "#         )\n",
    "#         fids = initial_fids\n",
    "#         callback_fun(self, fids, 0, 0)\n",
    "#         try:  # will catch keyboard inturrupt\n",
    "#             for epoch in range(self.parameters[\"epochs\"] + 1)[1:]:\n",
    "#                 for _ in range(self.parameters[\"epoch_size\"]):\n",
    "#                     with tf.GradientTape() as tape:\n",
    "#                         betas_rho = entry_stop_gradients(self.betas_rho, self.beta_mask)\n",
    "#                         betas_angle = entry_stop_gradients(\n",
    "#                             self.betas_angle, self.beta_mask\n",
    "#                         )\n",
    "#                         gammas_rho = entry_stop_gradients(self.gammas_rho, self.gamma_mask)\n",
    "#                         gammas_angle = entry_stop_gradients(\n",
    "#                             self.gammas_angle, self.gamma_mask\n",
    "#                         )\n",
    "#                         if self.parameters[\"use_displacements\"]:\n",
    "#                             alphas1_rho = entry_stop_gradients(\n",
    "#                                 self.alphas1_rho, self.alpha1_mask\n",
    "#                             )\n",
    "#                             alphas1_angle = entry_stop_gradients(\n",
    "#                                 self.alphas1_angle, self.alpha1_mask\n",
    "#                             )\n",
    "#                             alphas2_rho = entry_stop_gradients(\n",
    "#                                 self.alphas2_rho, self.alpha2_mask\n",
    "#                             )\n",
    "#                             alphas2_angle = entry_stop_gradients(\n",
    "#                                 self.alphas2_angle, self.alpha2_mask\n",
    "#                             )\n",
    "#                         else:\n",
    "#                             alphas1_rho = self.alphas1_rho\n",
    "#                             alphas1_angle = self.alphas1_angle\n",
    "#                             alphas2_rho = self.alphas2_rho\n",
    "#                             alphas2_angle = self.alphas2_angle\n",
    "#                         phis = entry_stop_gradients(self.phis, self.phi_mask)\n",
    "#                         if self.parameters[\"use_etas\"]:\n",
    "#                             etas = entry_stop_gradients(self.etas, self.eta_mask)\n",
    "#                         else:\n",
    "#                             etas = self.etas\n",
    "#                         thetas = entry_stop_gradients(self.thetas, self.theta_mask)\n",
    "#                         new_fids = self.batch_fidelities(\n",
    "#                             betas_rho,\n",
    "#                             betas_angle,\n",
    "#                             gammas_rho,\n",
    "#                             gammas_angle,\n",
    "#                             alphas1_rho,\n",
    "#                             alphas1_angle,\n",
    "#                             alphas2_rho,\n",
    "#                             alphas2_angle,\n",
    "#                             phis,\n",
    "#                             etas,\n",
    "#                             thetas,\n",
    "#                         )\n",
    "#                         new_loss = loss_fun(new_fids)\n",
    "#                         dloss_dvar = tape.gradient(new_loss, variables)\n",
    "#                     optimizer.apply_gradients(zip(dloss_dvar, variables))\n",
    "#                 dfids = new_fids - fids\n",
    "#                 fids = new_fids\n",
    "#                 callback_fun(self, fids, dfids, epoch)\n",
    "#                 condition_fid = tf.greater(fids, self.parameters[\"term_fid\"])\n",
    "#                 condition_dfid = tf.greater(dfids, self.parameters[\"dfid_stop\"])\n",
    "#                 if tf.reduce_any(condition_fid):\n",
    "#                     print(\"\\n\\n Optimization stopped. Term fidelity reached.\\n\")\n",
    "#                     termination_reason = \"term_fid\"\n",
    "#                     break\n",
    "#                 if not tf.reduce_any(condition_dfid):\n",
    "#                     print(\"\\n max dFid: %6f\" % tf.reduce_max(dfids).numpy())\n",
    "#                     print(\"dFid stop: %6f\" % self.parameters[\"dfid_stop\"])\n",
    "#                     print(\n",
    "#                         \"\\n\\n Optimization stopped.  No dfid is greater than dfid_stop\\n\"\n",
    "#                     )\n",
    "#                     termination_reason = \"dfid\"\n",
    "#                     break\n",
    "#         except KeyboardInterrupt:\n",
    "#             print(\"\\n max dFid: %6f\" % tf.reduce_max(dfids).numpy())\n",
    "#             print(\"dFid stop: %6f\" % self.parameters[\"dfid_stop\"])\n",
    "#             print(\"\\n\\n Optimization stopped on keyboard interrupt\")\n",
    "#             termination_reason = \"keyboard_interrupt\"\n",
    "\n",
    "#         if epoch == self.parameters[\"epochs\"]:\n",
    "#             termination_reason = \"epochs\"\n",
    "#             print(\n",
    "#                 \"\\n\\nOptimization stopped.  Reached maximum number of epochs. Terminal fidelity not reached.\\n\"\n",
    "#             )\n",
    "#         self._save_termination_reason(timestamp, termination_reason)\n",
    "#         timestamp_end = datetime.datetime.now().strftime(TIMESTAMP_FORMAT)\n",
    "#         elapsed_time_s = time.time() - start_time\n",
    "#         epoch_time_s = elapsed_time_s / epoch\n",
    "#         step_time_s = epoch_time_s / self.parameters[\"epochs\"]\n",
    "#         self.print_info()\n",
    "#         print(\"all data saved as: \" + self.filename)\n",
    "#         print(\"termination reason: \" + termination_reason)\n",
    "#         print(\"optimization timestamp (start time): \" + timestamp)\n",
    "#         print(\"timestamp (end time): \" + timestamp_end)\n",
    "#         print(\"elapsed time: \" + str(datetime.timedelta(seconds=elapsed_time_s)))\n",
    "#         print(\n",
    "#             \"Time per epoch (epoch size = %d): \" % self.parameters[\"epoch_size\"]\n",
    "#             + str(datetime.timedelta(seconds=epoch_time_s))\n",
    "#         )\n",
    "#         print(\n",
    "#             \"Time per Adam step (N_multistart = %d, N_cav1 = %d, N_cav2 = %d): \"\n",
    "#             % (self.parameters[\"N_multistart\"], self.parameters[\"N_cav1\"], self.parameters[\"N_cav2\"])\n",
    "#             + str(datetime.timedelta(seconds=step_time_s))\n",
    "#         )\n",
    "#         print(END_OPT_STRING)\n",
    "#         return timestamp\n",
    "\n",
    "    # if append is True, it will assume the dataset is already created and append only the\n",
    "    # last aquired values to it.\n",
    "    # TODO: if needed, could use compression when saving data.\n",
    "#     def _save_optimization_data(\n",
    "#         self,\n",
    "#         timestamp,\n",
    "#         fidelities_np,\n",
    "#         betas_np,\n",
    "#         gammas_np,\n",
    "#         alphas1_np,\n",
    "#         alphas2_np,\n",
    "#         phis_np,\n",
    "#         etas_np,\n",
    "#         thetas_np,\n",
    "#         elapsed_time_s,\n",
    "#         append,\n",
    "#     ):\n",
    "#         if not append:\n",
    "#             with h5py.File(self.filename, \"a\") as f:\n",
    "#                 grp = f.create_group(timestamp)\n",
    "#                 for parameter, value in self.parameters.items():\n",
    "#                     grp.attrs[parameter] = value\n",
    "#                 grp.attrs[\"termination_reason\"] = \"outside termination\"\n",
    "#                 grp.attrs[\"elapsed_time_s\"] = elapsed_time_s\n",
    "#                 if self.target_unitary is not None:\n",
    "#                     grp.create_dataset(\n",
    "#                         \"target_unitary\", data=self.target_unitary.numpy()\n",
    "#                     )\n",
    "#                 grp.create_dataset(\"initial_states\", data=self.initial_states.numpy())\n",
    "#                 grp.create_dataset(\"target_states\", data=self.target_states.numpy())\n",
    "#                 # dims = [[2, int(self.initial_states[0].numpy().shape[0] / 2)], [1, 1]]\n",
    "#                 grp.create_dataset(\n",
    "#                     \"fidelities\",\n",
    "#                     chunks=True,\n",
    "#                     data=[fidelities_np],\n",
    "#                     maxshape=(None, self.parameters[\"N_multistart\"]),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"betas\",\n",
    "#                     data=[betas_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(\n",
    "#                         None,\n",
    "#                         self.parameters[\"N_multistart\"],\n",
    "#                         self.parameters[\"N_blocks\"],\n",
    "#                     ),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"gammas\",\n",
    "#                     data=[gammas_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(\n",
    "#                         None,\n",
    "#                         self.parameters[\"N_multistart\"],\n",
    "#                         self.parameters[\"N_blocks\"],\n",
    "#                     ),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"alphas1\",\n",
    "#                     data=[alphas1_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(None, self.parameters[\"N_multistart\"], 1,),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"alphas2\",\n",
    "#                     data=[alphas2_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(None, self.parameters[\"N_multistart\"], 1,),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"phis\",\n",
    "#                     data=[phis_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(\n",
    "#                         None,\n",
    "#                         self.parameters[\"N_multistart\"],\n",
    "#                         self.parameters[\"N_blocks\"],\n",
    "#                     ),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"etas\",\n",
    "#                     data=[etas_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(\n",
    "#                         None,\n",
    "#                         self.parameters[\"N_multistart\"],\n",
    "#                         self.parameters[\"N_blocks\"],\n",
    "#                     ),\n",
    "#                 )\n",
    "#                 grp.create_dataset(\n",
    "#                     \"thetas\",\n",
    "#                     data=[thetas_np],\n",
    "#                     chunks=True,\n",
    "#                     maxshape=(\n",
    "#                         None,\n",
    "#                         self.parameters[\"N_multistart\"],\n",
    "#                         self.parameters[\"N_blocks\"],\n",
    "#                     ),\n",
    "#                 )\n",
    "#         else:  # just append the data\n",
    "#             with h5py.File(self.filename, \"a\") as f:\n",
    "#                 f[timestamp][\"fidelities\"].resize(\n",
    "#                     f[timestamp][\"fidelities\"].shape[0] + 1, axis=0\n",
    "#                 )\n",
    "#                 f[timestamp][\"betas\"].resize(f[timestamp][\"betas\"].shape[0] + 1, axis=0)\n",
    "#                 f[timestamp][\"gammas\"].resize(f[timestamp][\"gammas\"].shape[0] + 1, axis=0)\n",
    "#                 f[timestamp][\"alphas1\"].resize(\n",
    "#                     f[timestamp][\"alphas1\"].shape[0] + 1, axis=0\n",
    "#                 )\n",
    "#                 f[timestamp][\"alphas2\"].resize(\n",
    "#                     f[timestamp][\"alphas2\"].shape[0] + 1, axis=0\n",
    "#                 )\n",
    "#                 f[timestamp][\"phis\"].resize(f[timestamp][\"phis\"].shape[0] + 1, axis=0)\n",
    "#                 f[timestamp][\"etas\"].resize(f[timestamp][\"etas\"].shape[0] + 1, axis=0)\n",
    "#                 f[timestamp][\"thetas\"].resize(\n",
    "#                     f[timestamp][\"thetas\"].shape[0] + 1, axis=0\n",
    "#                 )\n",
    "\n",
    "#                 f[timestamp][\"fidelities\"][-1] = fidelities_np\n",
    "#                 f[timestamp][\"betas\"][-1] = betas_np\n",
    "#                 f[timestamp][\"gammas\"][-1] = gammas_np\n",
    "#                 f[timestamp][\"alphas1\"][-1] = alphas1_np\n",
    "#                 f[timestamp][\"alphas2\"][-1] = alphas2_np\n",
    "#                 f[timestamp][\"phis\"][-1] = phis_np\n",
    "#                 f[timestamp][\"etas\"][-1] = etas_np\n",
    "#                 f[timestamp][\"thetas\"][-1] = thetas_np\n",
    "#                 f[timestamp].attrs[\"elapsed_time_s\"] = elapsed_time_s\n",
    "\n",
    "#     def _save_termination_reason(self, timestamp, termination_reason):\n",
    "#         with h5py.File(self.filename, \"a\") as f:\n",
    "#             f[timestamp].attrs[\"termination_reason\"] = termination_reason\n",
    "\n",
    "    def randomize_and_set_vars(self):\n",
    "        beta_scale = self.parameters[\"beta_scale\"]\n",
    "        gamma_scale = self.parameters[\"gamma_scale\"]\n",
    "        alpha1_scale = self.parameters[\"alpha1_scale\"]\n",
    "        alpha2_scale = self.parameters[\"alpha2_scale\"]\n",
    "        theta_scale = self.parameters[\"theta_scale\"]\n",
    "        betas_rho = np.random.uniform(\n",
    "            0,\n",
    "            beta_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        betas_angle = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        gammas_rho = np.random.uniform(\n",
    "            0,\n",
    "            gamma_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        gammas_angle = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        if self.parameters[\"use_displacements\"]:\n",
    "            alphas1_rho = np.random.uniform(\n",
    "                0, alpha1_scale, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "            alphas1_angle = np.random.uniform(\n",
    "                -np.pi, np.pi, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "            alphas2_rho = np.random.uniform(\n",
    "                0, alpha2_scale, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "            alphas2_angle = np.random.uniform(\n",
    "                -np.pi, np.pi, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "        phis = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        if self.parameters[\"use_etas\"]:  # eta range is 0 to pi.\n",
    "            etas = np.random.uniform(\n",
    "                -np.pi,\n",
    "                np.pi,\n",
    "                size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "        thetas = np.random.uniform(\n",
    "            -1 * theta_scale,\n",
    "            theta_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        phis[0] = 0  # everything is relative to first phi\n",
    "        if self.parameters[\"no_CD_end\"]:\n",
    "            betas_rho[-1] = 0\n",
    "            betas_angle[-1] = 0\n",
    "            gammas_rho[-1] = 0\n",
    "            gammas_angle[-1] = 0\n",
    "        self.betas_rho = tf.Variable(\n",
    "            betas_rho, dtype=tf.float32, trainable=True, name=\"betas_rho\",\n",
    "        )\n",
    "        self.betas_angle = tf.Variable(\n",
    "            betas_angle, dtype=tf.float32, trainable=True, name=\"betas_angle\",\n",
    "        )\n",
    "        self.gammas_rho = tf.Variable(\n",
    "            gammas_rho, dtype=tf.float32, trainable=True, name=\"gammas_rho\",\n",
    "        )\n",
    "        self.gammas_angle = tf.Variable(\n",
    "            gammas_angle, dtype=tf.float32, trainable=True, name=\"gammas_angle\",\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if self.parameters[\"use_displacements\"]:\n",
    "            self.alphas1_rho = tf.Variable(\n",
    "                alphas1_rho, dtype=tf.float32, trainable=True, name=\"alphas1_rho\",\n",
    "            )\n",
    "            self.alphas1_angle = tf.Variable(\n",
    "                alphas1_angle, dtype=tf.float32, trainable=True, name=\"alphas1_angle\",\n",
    "            )\n",
    "            self.alphas2_rho = tf.Variable(\n",
    "                alphas2_rho, dtype=tf.float32, trainable=True, name=\"alphas2_rho\",\n",
    "            )\n",
    "            self.alphas2_angle = tf.Variable(\n",
    "                alphas2_angle, dtype=tf.float32, trainable=True, name=\"alphas2_angle\",\n",
    "            )\n",
    "        else:\n",
    "            self.alphas1_rho = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            self.alphas1_angle = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            self.alphas2_rho = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            self.alphas2_angle = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "        self.phis = tf.Variable(phis, dtype=tf.float32, trainable=True, name=\"phis\",)\n",
    "        if self.parameters[\"use_etas\"]:\n",
    "            self.etas = tf.Variable(\n",
    "                etas, dtype=tf.float32, trainable=True, name=\"etas\",\n",
    "            )\n",
    "        else:\n",
    "            self.etas = tf.constant(\n",
    "                (np.pi / 2.0) * np.ones_like(phis), dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        self.thetas = tf.Variable(\n",
    "            thetas, dtype=tf.float32, trainable=True, name=\"thetas\",\n",
    "        )\n",
    "\n",
    "    def get_numpy_vars(\n",
    "        self,\n",
    "        betas_rho=None,\n",
    "        betas_angle=None,\n",
    "        gammas_rho=None,\n",
    "        gammas_angle=None,\n",
    "        alphas1_rho=None,\n",
    "        alphas1_angle=None,\n",
    "        alphas2_rho=None,\n",
    "        alphas2_angle=None,\n",
    "        phis=None,\n",
    "        etas=None,\n",
    "        thetas=None,\n",
    "    ):\n",
    "        betas_rho = self.betas_rho if betas_rho is None else betas_rho\n",
    "        betas_angle = self.betas_angle if betas_angle is None else betas_angle\n",
    "        gammas_rho = self.gammas_rho if gammas_rho is None else gammas_rho\n",
    "        gammas_angle = self.gammas_angle if gammas_angle is None else gammas_angle\n",
    "        alphas1_rho = self.alphas1_rho if alphas1_rho is None else alphas1_rho\n",
    "        alphas1_angle = self.alphas1_angle if alphas1_angle is None else alphas1_angle\n",
    "        alphas2_rho = self.alphas2_rho if alphas2_rho is None else alphas2_rho\n",
    "        alphas2_angle = self.alphas2_angle if alphas2_angle is None else alphas2_angle\n",
    "        phis = self.phis if phis is None else phis\n",
    "        etas = self.etas if etas is None else etas\n",
    "        thetas = self.thetas if thetas is None else thetas\n",
    "\n",
    "        betas = betas_rho.numpy() * np.exp(1j * betas_angle.numpy())\n",
    "        gammas = gammas_rho.numpy() * np.exp(1j * gammas_angle.numpy())\n",
    "        alphas1 = alphas1_rho.numpy() * np.exp(1j * alphas1_angle.numpy())\n",
    "        alphas2 = alphas2_rho.numpy() * np.exp(1j * alphas2_angle.numpy())\n",
    "        phis = phis.numpy()\n",
    "        etas = etas.numpy()\n",
    "        thetas = thetas.numpy()\n",
    "        # now, to wrap phis, etas, and thetas so it's in the range [-pi, pi]\n",
    "        phis = (phis + np.pi) % (2 * np.pi) - np.pi\n",
    "        etas = (etas + np.pi) % (2 * np.pi) - np.pi\n",
    "        thetas = (thetas + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "        # these will have shape N_multistart x N_blocks\n",
    "        return betas.T, gammas.T, alphas1.T, alphas2.T, phis.T, etas.T, thetas.T\n",
    "\n",
    "    def set_tf_vars(self, betas=None, gammas=None, alphas1=None, alphas2=None, phis=None, etas=None, thetas=None):\n",
    "        # reshaping for N_multistart = 1\n",
    "        if betas is not None:\n",
    "            if len(betas.shape) < 2:\n",
    "                betas = betas.reshape(betas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            betas_rho = np.abs(betas)\n",
    "            betas_angle = np.angle(betas)\n",
    "            self.betas_rho = tf.Variable(\n",
    "                betas_rho, dtype=tf.float32, trainable=True, name=\"betas_rho\"\n",
    "            )\n",
    "            self.betas_angle = tf.Variable(\n",
    "                betas_angle, dtype=tf.float32, trainable=True, name=\"betas_angle\",\n",
    "            )\n",
    "        if gammas is not None:\n",
    "            if len(gammas.shape) < 2:\n",
    "                gammas = gammas.reshape(betas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            gammas_rho = np.abs(gammas)\n",
    "            gammas_angle = np.angle(gammas)\n",
    "            self.gammas_rho = tf.Variable(\n",
    "                gammas_rho, dtype=tf.float32, trainable=True, name=\"gammas_rho\"\n",
    "            )\n",
    "            self.gammas_angle = tf.Variable(\n",
    "                gammas_angle, dtype=tf.float32, trainable=True, name=\"gammas_angle\",\n",
    "            )\n",
    "        if alphas1 is not None:\n",
    "            if len(alphas1.shape) < 2:\n",
    "                alphas1 = alphas1.reshape(alphas1.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            alphas1_rho = np.abs(alphas1)\n",
    "            alphas1_angle = np.angle(alphas1)\n",
    "         \n",
    "            if self.parameters[\"use_displacements\"]:\n",
    "                self.alphas1_rho = tf.Variable(\n",
    "                    alphas1_rho, dtype=tf.float32, trainable=True, name=\"alphas1_rho\",\n",
    "                )\n",
    "                self.alphas1_angle = tf.Variable(\n",
    "                    alphas1_angle, dtype=tf.float32, trainable=True, name=\"alphas1_angle\",\n",
    "                )\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                self.alphas1_rho = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                self.alphas1_angle = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                \n",
    "        if alphas2 is not None:\n",
    "            if len(alphas2.shape) < 2:\n",
    "                alphas2 = alphas2.reshape(alphas2.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            alphas2_rho = np.abs(alphas2)\n",
    "            alphas2_angle = np.angle(alphas2)\n",
    "            if self.parameters[\"use_displacements\"]:                \n",
    "                self.alphas2_rho = tf.Variable(\n",
    "                    alphas2_rho, dtype=tf.float32, trainable=True, name=\"alphas2_rho\",\n",
    "                )\n",
    "                self.alphas2_angle = tf.Variable(\n",
    "                    alphas2_angle, dtype=tf.float32, trainable=True, name=\"alphas2_angle\",\n",
    "                )\n",
    "            else:\n",
    "                self.alphas2_rho = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                self.alphas2_angle = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "\n",
    "\n",
    "        if phis is not None:\n",
    "            if len(phis.shape) < 2:\n",
    "                phis = phis.reshape(phis.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.phis = tf.Variable(\n",
    "                phis, dtype=tf.float32, trainable=True, name=\"phis\",\n",
    "            )\n",
    "        if etas is not None:\n",
    "            if len(etas.shape) < 2:\n",
    "                etas = etas.reshape(etas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.etas = tf.Variable(\n",
    "                etas, dtype=tf.float32, trainable=True, name=\"etas\",\n",
    "            )\n",
    "        if thetas is not None:\n",
    "            if len(thetas.shape) < 2:\n",
    "                thetas = thetas.reshape(thetas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.thetas = tf.Variable(\n",
    "                thetas, dtype=tf.float32, trainable=True, name=\"thetas\",\n",
    "            )\n",
    "\n",
    "    def best_circuit(self):\n",
    "        fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis,\n",
    "            self.etas,\n",
    "            self.thetas,\n",
    "        )\n",
    "        fids = np.atleast_1d(fids.numpy())\n",
    "        max_idx = np.argmax(fids)\n",
    "        all_betas, all_gammas,all_alphas1, all_alphas2, all_phis, all_etas, all_thetas = self.get_numpy_vars(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis,\n",
    "            self.etas,\n",
    "            self.thetas,\n",
    "        )\n",
    "        max_fid = fids[max_idx]\n",
    "        betas = all_betas[max_idx]\n",
    "        gammas = all_gammas[max_idx]\n",
    "        alphas1 = all_alphas1[max_idx]\n",
    "        alphas2 = all_alphas2[max_idx]\n",
    "        phis = all_phis[max_idx]\n",
    "        etas = all_etas[max_idx]\n",
    "        thetas = all_thetas[max_idx]\n",
    "        return {\n",
    "            \"fidelity\": max_fid,\n",
    "            \"betas\": betas,\n",
    "            \"gammas\": gammas,\n",
    "            \"alphas1\": alphas1,\n",
    "            \"alphas2\": alphas2,\n",
    "            \"phis\": phis,\n",
    "            \"etas\": etas,\n",
    "            \"thetas\": thetas,\n",
    "        }\n",
    "\n",
    "    def all_fidelities(self):\n",
    "        fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis,\n",
    "            self.etas,\n",
    "            self.thetas,\n",
    "        )\n",
    "        return fids.numpy()\n",
    "\n",
    "    def best_fidelity(self):\n",
    "        fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis,\n",
    "            self.etas,\n",
    "            self.thetas,\n",
    "        )\n",
    "        max_idx = tf.argmax(fids).numpy()\n",
    "        max_fid = fids[max_idx].numpy()\n",
    "        return max_fid\n",
    "\n",
    "    def print_info(self):\n",
    "        best_circuit = self.best_circuit()\n",
    "        with np.printoptions(precision=5, suppress=True):\n",
    "            for parameter, value in self.parameters.items():\n",
    "                print(parameter + \": \" + str(value))\n",
    "            print(\"filename: \" + self.filename)\n",
    "            print(\"\\nBest circuit parameters found:\")\n",
    "            print(\"betas:         \" + str(best_circuit[\"betas\"]))\n",
    "            print(\"gammas:         \" + str(best_circuit[\"gammas\"]))\n",
    "            print(\"alphas1:        \" + str(best_circuit[\"alphas1\"]))\n",
    "            print(\"alphas2:        \" + str(best_circuit[\"alphas2\"]))\n",
    "            print(\"phis (deg):    \" + str(best_circuit[\"phis\"] * 180.0 / np.pi))\n",
    "            print(\"etas (deg):    \" + str(best_circuit[\"etas\"] * 180.0 / np.pi))\n",
    "            print(\"thetas (deg):  \" + str(best_circuit[\"thetas\"] * 180.0 / np.pi))\n",
    "            print(\"Max Fidelity:  %.6f\" % best_circuit[\"fidelity\"])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2aad9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Optimize Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4941c632",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def optimize():\n",
    "        #print('optimize called')\n",
    "    timestamp = datetime.datetime.now().strftime(TIMESTAMP_FORMAT)\n",
    "    opt.timestamps.append(timestamp)\n",
    "    print(\"Start time: \" + timestamp)\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "    optimizer = tf.optimizers.SGD(opt.parameters[\"learning_rate\"], momentum = 0.9)\n",
    "    if opt.parameters[\"use_displacements\"] and opt.parameters[\"use_etas\"]:\n",
    "        variables = [\n",
    "            opt.betas_rho,\n",
    "            opt.betas_angle,\n",
    "            opt.gammas_rho,\n",
    "            opt.gammas_angle,\n",
    "            opt.alphas1_rho,\n",
    "            opt.alphas1_angle,\n",
    "            opt.alphas2_rho,\n",
    "            opt.alphas2_angle,\n",
    "            opt.phis,\n",
    "            opt.etas,\n",
    "            opt.thetas,\n",
    "        ]\n",
    "    elif opt.parameters[\"use_etas\"]:\n",
    "        variables = [\n",
    "            opt.betas_rho,\n",
    "            opt.betas_angle,\n",
    "            opt.gammas_rho,\n",
    "            opt.gammas_angle,\n",
    "            opt.phis,\n",
    "            opt.etas,\n",
    "            opt.thetas,\n",
    "        ]\n",
    "    elif opt.parameters[\"use_displacements\"]:\n",
    "        variables = [\n",
    "            opt.betas_rho,\n",
    "            opt.betas_angle,\n",
    "            opt.gammas_rho,\n",
    "            opt.gammas_angle,\n",
    "            opt.alphas1_rho,\n",
    "            opt.alphas1_angle,\n",
    "            opt.alphas2_rho,\n",
    "            opt.alphas2_angle,\n",
    "            opt.phis,\n",
    "            opt.thetas,\n",
    "        ]\n",
    "    else:\n",
    "        variables = [\n",
    "            opt.betas_rho,\n",
    "            opt.betas_angle,\n",
    "            opt.gammas_rho,\n",
    "            opt.gammas_angle,\n",
    "            opt.phis,\n",
    "            opt.thetas,\n",
    "        ]\n",
    "\n",
    "    @tf.function\n",
    "    def entry_stop_gradients(target, mask):\n",
    "        #print('entry stop grad called')\n",
    "        mask_h = tf.abs(mask - 1)\n",
    "        return tf.stop_gradient(mask_h * target) + mask * target\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def loss_fun(fids):\n",
    "        #print('loss fun called')\n",
    "        # I think it's important that the log is taken before the avg\n",
    "        losses = tf.math.log(1 - fids)\n",
    "        avg_loss = tf.reduce_sum(losses) / opt.parameters[\"N_multistart\"]\n",
    "        return avg_loss\n",
    "\n",
    "    def callback_fun(obj, fids, dfids, epoch):\n",
    "        #print('callback fun called')\n",
    "        elapsed_time_s = time.time() - start_time\n",
    "        time_per_epoch = elapsed_time_s / epoch if epoch != 0 else 0.0\n",
    "        epochs_left = opt.parameters[\"epochs\"] - epoch\n",
    "        expected_time_remaining = epochs_left * time_per_epoch\n",
    "        fidelities_np = np.squeeze(np.array(fids))\n",
    "        betas_np, gammas_np, alphas1_np, alphas2_np, phis_np, etas_np, thetas_np = opt.get_numpy_vars()\n",
    "\n",
    "        avg_fid = tf.reduce_sum(fids) / opt.parameters[\"N_multistart\"]\n",
    "        max_fid = tf.reduce_max(fids)\n",
    "        avg_dfid = tf.reduce_sum(dfids) / opt.parameters[\"N_multistart\"]\n",
    "        max_dfid = tf.reduce_max(dfids)\n",
    "        extra_string = \" (real part)\" if opt.parameters[\"use_phase\"] else \"\"\n",
    "        if do_prints:\n",
    "            print(\n",
    "                \"\\r Epoch: %d / %d Max Fid: %.6f Avg Fid: %.6f Max dFid: %.6f Avg dFid: %.6f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    opt.parameters[\"epochs\"],\n",
    "                    max_fid,\n",
    "                    avg_fid,\n",
    "                    max_dfid,\n",
    "                    avg_dfid,\n",
    "                )\n",
    "                + \" Elapsed time: \"\n",
    "                + str(datetime.timedelta(seconds=elapsed_time_s))\n",
    "                + \" Remaing time: \"\n",
    "                + str(datetime.timedelta(seconds=expected_time_remaining))\n",
    "                + extra_string,\n",
    "                end=\"\",\n",
    "            )\n",
    "\n",
    "    initial_fids = opt.batch_fidelities(\n",
    "        opt.betas_rho,\n",
    "        opt.betas_angle,\n",
    "        opt.gammas_rho,\n",
    "        opt.gammas_angle,\n",
    "        opt.alphas1_rho,\n",
    "        opt.alphas1_angle,\n",
    "        opt.alphas2_rho,\n",
    "        opt.alphas2_angle,\n",
    "        opt.phis,\n",
    "        opt.etas,\n",
    "        opt.thetas,\n",
    "    )\n",
    "    fids = initial_fids\n",
    "    callback_fun(opt, fids, 0, 0)\n",
    "    try:  # will catch keyboard inturrupt\n",
    "        for epoch in range(opt.parameters[\"epochs\"] + 1)[1:]:\n",
    "            for _ in range(opt.parameters[\"epoch_size\"]):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    betas_rho = entry_stop_gradients(opt.betas_rho, opt.beta_mask)\n",
    "                    betas_angle = entry_stop_gradients(\n",
    "                        opt.betas_angle, opt.beta_mask\n",
    "                    )\n",
    "                    gammas_rho = entry_stop_gradients(opt.gammas_rho, opt.gamma_mask)\n",
    "                    gammas_angle = entry_stop_gradients(\n",
    "                        opt.gammas_angle, opt.gamma_mask\n",
    "                    )\n",
    "                    if opt.parameters[\"use_displacements\"]:\n",
    "                        alphas1_rho = entry_stop_gradients(\n",
    "                            opt.alphas1_rho, opt.alpha1_mask\n",
    "                        )\n",
    "                        alphas1_angle = entry_stop_gradients(\n",
    "                            opt.alphas1_angle, opt.alpha1_mask\n",
    "                        )\n",
    "                        alphas2_rho = entry_stop_gradients(\n",
    "                            opt.alphas2_rho, opt.alpha2_mask\n",
    "                        )\n",
    "                        alphas2_angle = entry_stop_gradients(\n",
    "                            opt.alphas2_angle, opt.alpha2_mask\n",
    "                        )\n",
    "                    else:\n",
    "                        alphas1_rho = opt.alphas1_rho\n",
    "                        alphas1_angle = opt.alphas1_angle\n",
    "                        alphas2_rho = opt.alphas2_rho\n",
    "                        alphas2_angle = opt.alphas2_angle\n",
    "                    phis = entry_stop_gradients(opt.phis, opt.phi_mask)\n",
    "                    if opt.parameters[\"use_etas\"]:\n",
    "                        etas = entry_stop_gradients(opt.etas, opt.eta_mask)\n",
    "                    else:\n",
    "                        etas = opt.etas\n",
    "                    thetas = entry_stop_gradients(opt.thetas, opt.theta_mask)\n",
    "                    new_fids = opt.batch_fidelities(\n",
    "                        betas_rho,\n",
    "                        betas_angle,\n",
    "                        gammas_rho,\n",
    "                        gammas_angle,\n",
    "                        alphas1_rho,\n",
    "                        alphas1_angle,\n",
    "                        alphas2_rho,\n",
    "                        alphas2_angle,\n",
    "                        phis,\n",
    "                        etas,\n",
    "                        thetas,\n",
    "                    )\n",
    "                    new_loss = loss_fun(new_fids)\n",
    "                    dloss_dvar = tape.gradient(new_loss, variables)\n",
    "                optimizer.apply_gradients(zip(dloss_dvar, variables))\n",
    "            dfids = new_fids - fids\n",
    "            fids = new_fids\n",
    "            callback_fun(opt, fids, dfids, epoch)\n",
    "            condition_fid = tf.greater(fids, opt.parameters[\"term_fid\"])\n",
    "            condition_dfid = tf.greater(dfids, opt.parameters[\"dfid_stop\"])\n",
    "            if tf.reduce_any(condition_fid):\n",
    "                print(\"\\n\\n Optimization stopped. Term fidelity reached.\\n\")\n",
    "                termination_reason = \"term_fid\"\n",
    "                break\n",
    "            if not tf.reduce_any(condition_dfid):\n",
    "                print(\"\\n max dFid: %6f\" % tf.reduce_max(dfids).numpy())\n",
    "                print(\"dFid stop: %6f\" % opt.parameters[\"dfid_stop\"])\n",
    "                print(\n",
    "                    \"\\n\\n Optimization stopped.  No dfid is greater than dfid_stop\\n\"\n",
    "                )\n",
    "                termination_reason = \"dfid\"\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n max dFid: %6f\" % tf.reduce_max(dfids).numpy())\n",
    "        print(\"dFid stop: %6f\" % opt.parameters[\"dfid_stop\"])\n",
    "        print(\"\\n\\n Optimization stopped on keyboard interrupt\")\n",
    "        termination_reason = \"keyboard_interrupt\"\n",
    "\n",
    "    if epoch == opt.parameters[\"epochs\"]:\n",
    "        termination_reason = \"epochs\"\n",
    "        print(\n",
    "            \"\\n\\nOptimization stopped.  Reached maximum number of epochs. Terminal fidelity not reached.\\n\"\n",
    "        )\n",
    "    #opt._save_termination_reason(timestamp, termination_reason)\n",
    "    timestamp_end = datetime.datetime.now().strftime(TIMESTAMP_FORMAT)\n",
    "    elapsed_time_s = time.time() - start_time\n",
    "    epoch_time_s = elapsed_time_s / epoch\n",
    "    step_time_s = epoch_time_s / opt.parameters[\"epochs\"]\n",
    "    opt.print_info()\n",
    "    print(\"all data saved as: \" + opt.filename)\n",
    "    print(\"termination reason: \" + termination_reason)\n",
    "    print(\"optimization timestamp (start time): \" + timestamp)\n",
    "    print(\"timestamp (end time): \" + timestamp_end)\n",
    "    print(\"elapsed time: \" + str(datetime.timedelta(seconds=elapsed_time_s)))\n",
    "    print(\n",
    "        \"Time per epoch (epoch size = %d): \" % opt.parameters[\"epoch_size\"]\n",
    "        + str(datetime.timedelta(seconds=epoch_time_s))\n",
    "    )\n",
    "    print(\n",
    "        \"Time per Adam step (N_multistart = %d, N_cav1 = %d, N_cav2 = %d): \"\n",
    "        % (opt.parameters[\"N_multistart\"], opt.parameters[\"N_cav1\"], opt.parameters[\"N_cav2\"])\n",
    "        + str(datetime.timedelta(seconds=step_time_s))\n",
    "    )\n",
    "    print(END_OPT_STRING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe51591",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972eaf4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Specify # of opt runs, multistart (batch size), etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13032e53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "do_prints = True\n",
    "opt_params['epochs'] = 10000\n",
    "opt_params['N_multistart'] = 100\n",
    "N_times = 30 #number of times \n",
    "learning_rates = [0.001, 0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82560511",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is 1\n",
      "Opt Run #2/2\n",
      "Start time: 2022-05-18 09:20:49\n",
      " Epoch: 8 / 10 Max Fid: 0.246123 Avg Fid: 0.216360 Max dFid: 0.000001 Avg dFid: 0.000000 Elapsed time: 0:00:17.457878 Remaing time: 0:00:04.364469\n",
      " max dFid: 0.000001\n",
      "dFid stop: 0.000001\n",
      "\n",
      "\n",
      " Optimization stopped.  No dfid is greater than dfid_stop\n",
      "\n",
      "optimization_type: state transfer\n",
      "N_multistart: 2\n",
      "N_blocks: 5\n",
      "term_fid: 0.995\n",
      "dfid_stop: 1e-06\n",
      "no_CD_end: False\n",
      "learning_rate: 1\n",
      "epoch_size: 20\n",
      "epochs: 10\n",
      "beta_scale: 3.0\n",
      "gamma_scale: 3.0\n",
      "alpha1_scale: 1.0\n",
      "alpha2_scale: 1.0\n",
      "theta_scale: 3.141592653589793\n",
      "use_etas: False\n",
      "use_displacements: False\n",
      "use_phase: False\n",
      "name: Fock1 1\n",
      "comment: \n",
      "N_cav1: 10\n",
      "N_cav2: 10\n",
      "filename: Fock1 1.h5\n",
      "\n",
      "Best circuit parameters found:\n",
      "betas:         [0.05193-0.40666j 1.75068-0.43679j 1.26828+0.4346j  0.08197+0.02465j\n",
      " 1.20245+1.55061j]\n",
      "gammas:         [-0.69415+2.68613j  1.22602-0.24541j  0.25142-0.14179j  0.38383-1.10991j\n",
      "  0.85415+0.48053j]\n",
      "alphas1:        [0.+0.j]\n",
      "alphas2:        [0.+0.j]\n",
      "phis (deg):    [   0.      -140.50276   63.26303  100.62006   70.03732]\n",
      "etas (deg):    [89.99999 89.99999 89.99999 89.99999 89.99999]\n",
      "thetas (deg):  [   0.01078 -179.98949  179.98401 -179.94696    0.02672]\n",
      "Max Fidelity:  0.246123\n",
      "\n",
      "\n",
      "all data saved as: Fock1 1.h5\n",
      "termination reason: dfid\n",
      "optimization timestamp (start time): 2022-05-18 09:20:49\n",
      "timestamp (end time): 2022-05-18 09:21:06\n",
      "elapsed time: 0:00:17.457878\n",
      "Time per epoch (epoch size = 20): 0:00:02.182235\n",
      "Time per Adam step (N_multistart = 2, N_cav1 = 10, N_cav2 = 10): 0:00:00.218223\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "circs = []\n",
    "fids = []\n",
    "\n",
    "for l in learning_rates:\n",
    "    #print('**learning rate is ')\n",
    "    opt_params['learning_rate'] = l\n",
    "        \n",
    "    fids_l = []\n",
    "    circs_l = []\n",
    "    for i in range(N_times):\n",
    "        opt = BatchOptimizer(**opt_params)\n",
    "        display.clear_output(wait=True)\n",
    "        print('learning rate is '+ str(l))\n",
    "        print('Opt Run #' + str(i+1) + \"/\" + str(N_times))\n",
    "        optimize()\n",
    "        fids_l.append(opt.best_fidelity())\n",
    "        circs_l.append(opt.best_circuit())\n",
    "    fids.append(fids_l)\n",
    "    circs_l.append(circs_l)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b99c36",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67bb5dc1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data):\n",
    "    '''\n",
    "    Input: List of numbers\n",
    "    Output: 1/2 size of error bar\n",
    "    '''\n",
    "    m = np.mean(data)\n",
    "    tot_diff_sq = 0\n",
    "    for i in data: \n",
    "        tot_diff_sq += (i-m)**2\n",
    "    mse = np.sqrt(tot_diff_sq/len(data))\n",
    "    return mse/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dc2ab6c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = []\n",
    "fid_plot = []\n",
    "\n",
    "for rate in range(len(learning_rates)):\n",
    "    err = compute_error(fids[rate])\n",
    "    errors.append(err)\n",
    "    fid_plot.append(np.mean(fids[rate]))\n",
    "    #fid_arr.append(fid_layers)\n",
    "\n",
    "#we prepare plot to track this stuff live--------\n",
    "plt.errorbar(learning_rates, fid_plot, yerr = errors, capsize=10, linestyle = '--', marker = 'o')\n",
    "\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Fidelity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95caf146",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b331b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "circs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

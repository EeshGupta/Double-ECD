{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c903249b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a005b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('C:\\\\Users\\\\Eesh Gupta\\\\Documents\\\\RU Research\\\\Chakram')\n",
    "#sys.path.append('C:\\\\Users\\\\Eesh Gupta\\\\Documents\\\\RU Research\\\\Chakram\\\\Double-ECD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3501bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Need tf version 2.3.0 or later. Using tensorflow version: 2.7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# note: timestamp can't use \"/\" character for h5 saving.\n",
    "TIMESTAMP_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "END_OPT_STRING = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # supress warnings\n",
    "import h5py\n",
    "\n",
    "print(\n",
    "    \"\\nNeed tf version 2.3.0 or later. Using tensorflow version: \"\n",
    "    + tf.__version__\n",
    "    + \"\\n\"\n",
    ")\n",
    "import ECD_control.ECD_optimization.tf_quantum as tfq\n",
    "from ECD_control.ECD_optimization.visualization import VisualizationMixin\n",
    "import qutip as qt\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83aa23",
   "metadata": {},
   "source": [
    "# Batch Optimizer Code \n",
    "To be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38c63f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# note: timestamp can't use \"/\" character for h5 saving.\n",
    "TIMESTAMP_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "END_OPT_STRING = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # supress warnings\n",
    "import h5py\n",
    "\n",
    "# print(\n",
    "#     \"\\nNeed tf version 2.3.0 or later. Using tensorflow version: \"\n",
    "#     + tf.__version__\n",
    "#     + \"\\n\"\n",
    "# )\n",
    "import ECD_control.ECD_optimization.tf_quantum as tfq\n",
    "from ECD_control.ECD_optimization.visualization import VisualizationMixin\n",
    "import qutip as qt\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def print_hi():\n",
    "    print('hi')\n",
    "\n",
    "\n",
    "\n",
    "class BatchOptimizer(VisualizationMixin):\n",
    "\n",
    "    # a block is defined as the unitary: CD(beta)D(alpha)R_phi(theta)\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimization_type=\"state transfer\",\n",
    "        target_unitary=None,\n",
    "        P_cav1=None,\n",
    "        P_cav2 = None, #EG\n",
    "        N_cav1=None,\n",
    "        N_cav2 = None, #EG\n",
    "        initial_states=None,\n",
    "        target_states=None,\n",
    "        expectation_operators=None,\n",
    "        target_expectation_values=None,\n",
    "        N_multistart=10,\n",
    "        N_blocks=20,\n",
    "        term_fid=0.99,  # can set >1 to force run all epochs\n",
    "        dfid_stop=1e-4,  # can be set= -1 to force run all epochs\n",
    "        learning_rate=0.01,\n",
    "        epoch_size=10,\n",
    "        epochs=100,\n",
    "        beta_scale=1.0,\n",
    "        gamma_scale = 1.0, #EG\n",
    "        alpha1_scale=1.0,\n",
    "        alpha2_scale=1.0,\n",
    "        theta_scale=np.pi,\n",
    "        use_etas=False,\n",
    "        use_displacements=False,\n",
    "        no_CD_end=False,\n",
    "        beta_mask=None,\n",
    "        gamma_mask = None, \n",
    "        phi1_mask = None,\n",
    "        phi2_mask = None,\n",
    "        eta_mask=None,\n",
    "        theta1_mask=None,\n",
    "        theta2_mask=None,\n",
    "        alpha1_mask=None,\n",
    "        alpha2_mask=None,\n",
    "        name=\"ECD_control\",\n",
    "        filename=None,\n",
    "        comment=\"\",\n",
    "        use_phase=False,  # include the phase in the optimization cost function. Important for unitaries.\n",
    "        timestamps=[],\n",
    "        **kwargs\n",
    "    ):\n",
    "        '''\n",
    "        Calls the following functions:\n",
    "        \n",
    "        construct needed matrices called\n",
    "        construct opt masks called\n",
    "        batch state transfer fids real part called\n",
    "        construct block ops batch called\n",
    "        construct displacement ops batch called\n",
    "        construct displacement ops batch called\n",
    "        '''\n",
    "        self.parameters = {\n",
    "            \"optimization_type\": optimization_type,\n",
    "            \"N_multistart\": N_multistart,\n",
    "            \"N_blocks\": N_blocks,\n",
    "            \"term_fid\": term_fid,\n",
    "            \"dfid_stop\": dfid_stop,\n",
    "            \"no_CD_end\": no_CD_end,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"epoch_size\": epoch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"beta_scale\": beta_scale,\n",
    "            \"gamma_scale\": gamma_scale, #EG\n",
    "            \"alpha1_scale\": alpha1_scale,\n",
    "            \"alpha2_scale\": alpha2_scale,\n",
    "            \"theta_scale\": theta_scale,\n",
    "            \"use_etas\": use_etas,\n",
    "            \"use_displacements\": use_displacements,\n",
    "            \"use_phase\": use_phase,\n",
    "            \"name\": name,\n",
    "            \"comment\": comment,\n",
    "        }\n",
    "        self.parameters.update(kwargs)\n",
    "        if (\n",
    "            self.parameters[\"optimization_type\"] == \"state transfer\"\n",
    "            or self.parameters[\"optimization_type\"] == \"analysis\"\n",
    "        ):\n",
    "            self.batch_fidelities = (\n",
    "                self.batch_state_transfer_fidelities\n",
    "                if self.parameters[\"use_phase\"]\n",
    "                else self.batch_state_transfer_fidelities_real_part\n",
    "            )\n",
    "            # set fidelity function\n",
    "\n",
    "            self.initial_states = tf.stack(\n",
    "                [tfq.qt2tf(state) for state in initial_states]\n",
    "            )\n",
    "\n",
    "            self.target_unitary = tfq.qt2tf(target_unitary)\n",
    "\n",
    "            # if self.target_unitary is not None: TODO\n",
    "            #     raise Exception(\"Need to fix target_unitary multi-state transfer generation!\")\n",
    "\n",
    "            self.target_states = (  # store dag\n",
    "                tf.stack([tfq.qt2tf(state) for state in target_states])\n",
    "                if self.target_unitary is None\n",
    "                else self.target_unitary @ self.initial_states\n",
    "            )\n",
    "\n",
    "            self.target_states_dag = tf.linalg.adjoint(\n",
    "                self.target_states\n",
    "            )  # store dag to avoid having to take adjoint\n",
    "\n",
    "            #N_cav1 = self.initial_states[0].numpy().shape[0] // 2\n",
    "#         elif self.parameters[\"optimization_type\"] == \"unitary\":\n",
    "#             self.target_unitary = tfq.qt2tf(target_unitary)\n",
    "#             N_cav1 = self.target_unitary.numpy().shape[0] // 2\n",
    "#             P_cav = P_cav if P_cav is not None else N_cav\n",
    "#             raise Exception(\"Need to implement unitary optimization\")\n",
    "\n",
    "        elif self.parameters[\"optimization_type\"] == \"expectation\":\n",
    "            raise Exception(\"Need to implement expectation optimization\")\n",
    "        elif (\n",
    "            self.parameters[\"optimization_type\"] == \"calculation\"\n",
    "        ):  # using functions but not doing opt\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"optimization_type must be one of {'state transfer', 'unitary', 'expectation', 'analysis', 'calculation'}\"\n",
    "            )\n",
    "\n",
    "        self.parameters[\"N_cav1\"] = N_cav1\n",
    "        self.parameters[\"N_cav2\"] = N_cav2\n",
    "#         if P_cav is not None:\n",
    "#             self.parameters[\"P_cav1\"] = P_cav1\n",
    "#             self.parameters[\"P_cav2\"] = P_cav2\n",
    "\n",
    "        # TODO: handle case when you pass initial params. In that case, don't randomize, but use \"set_tf_vars()\"\n",
    "        self.randomize_and_set_vars()\n",
    "        # self.set_tf_vars(betas=betas, alphas=alphas, phis=phis, thetas=thetas)\n",
    "\n",
    "        self._construct_needed_matrices()\n",
    "        self._construct_optimization_masks(beta_mask,gamma_mask, alpha1_mask, alpha2_mask, phi1_mask,phi2_mask,eta_mask, theta1_mask, theta2_mask)\n",
    "\n",
    "        # opt data will be a dictionary of dictonaries used to store optimization data\n",
    "        # the dictionary will be addressed by timestamps of optmization.\n",
    "        # each opt will append to opt_data a dictionary\n",
    "        # this dictionary will contain optimization parameters and results\n",
    "\n",
    "        self.timestamps = timestamps\n",
    "        self.filename = (\n",
    "            filename\n",
    "            if (filename is not None and filename != \"\")\n",
    "            else self.parameters[\"name\"]\n",
    "        )\n",
    "        path = self.filename.split(\".\")\n",
    "        if len(path) < 2 or (len(path) == 2 and path[-1] != \".h5\"):\n",
    "            self.filename = path[0] + \".h5\"\n",
    "\n",
    "#     def modify_parameters(self, **kwargs):\n",
    "#         print('modify parameters called')\n",
    "#         # currently, does not support changing optimization type.\n",
    "#         # todo: update for multi-state optimization and unitary optimziation\n",
    "#         parameters = kwargs\n",
    "#         for param, value in self.parameters.items():\n",
    "#             if param not in parameters:\n",
    "#                 parameters[param] = value\n",
    "#         # handle things that are not in self.parameters:\n",
    "#         parameters[\"initial_states\"] = (\n",
    "#             parameters[\"initial_states\"]\n",
    "#             if \"initial_states\" in parameters\n",
    "#             else self.initial_states\n",
    "#         )\n",
    "#         parameters[\"target_states\"] = (\n",
    "#             parameters[\"target_states\"]\n",
    "#             if \"target_states\" in parameters\n",
    "#             else self.target_states\n",
    "#         )\n",
    "#         parameters[\"filename\"] = (\n",
    "#             parameters[\"filename\"] if \"filename\" in parameters else self.filename\n",
    "#         )\n",
    "#         parameters[\"timestamps\"] = (\n",
    "#             parameters[\"timestamps\"] if \"timestamps\" in parameters else self.timestamps\n",
    "#         )\n",
    "#         self.__init__(**parameters)\n",
    "\n",
    "    def _construct_needed_matrices(self):\n",
    "        \"\"\"\n",
    "        EG: this function is making the a, a_dagger matrices for the cavity modes\n",
    "        and these functions will be later exponentiated to get the displacement operator\n",
    "        \n",
    "        Edited to inclide 2nd mode\n",
    "        \"\"\"\n",
    "        #print('construct needed matrices called')\n",
    "        N_cav1 = self.parameters[\"N_cav1\"]\n",
    "        N_cav2 = self.parameters[\"N_cav2\"]\n",
    "        q1 = tfq.position(N_cav1)\n",
    "        p1 = tfq.momentum(N_cav1)\n",
    "        q2 = tfq.position(N_cav2)\n",
    "        p2 = tfq.momentum(N_cav2)\n",
    "\n",
    "        # Pre-diagonalize\n",
    "        (self._eig_q1, self._U_q1) = tf.linalg.eigh(q1)\n",
    "        (self._eig_p1, self._U_p1) = tf.linalg.eigh(p1)\n",
    "        (self._eig_q2, self._U_q2) = tf.linalg.eigh(q2)\n",
    "        (self._eig_p2, self._U_p2) = tf.linalg.eigh(p2)\n",
    "\n",
    "        self._qp1_comm = tf.linalg.diag_part(q1 @ p1 - p1 @ q1)\n",
    "        self._qp2_comm = tf.linalg.diag_part(q2 @ p2 - p2 @ q2)\n",
    "        #first mode\n",
    "        if self.parameters[\"optimization_type\"] == \"unitary\":\n",
    "            P_cav1 = self.parameters[\"P_cav1\"]\n",
    "            partial_I1 = np.array(qt.identity(N_cav1))\n",
    "            for j in range(P_cav1, N_cav1):\n",
    "                partial_I1[j, j] = 0\n",
    "            partial_I1 = qt.Qobj(partial_I1)\n",
    "            self.P1_matrix = tfq.qt2tf(qt.tensor(qt.identity(2), partial_I1))\n",
    "        #second mode\n",
    "        if self.parameters[\"optimization_type\"] == \"unitary\":\n",
    "            P_cav2 = self.parameters[\"P_cav2\"]\n",
    "            partial_I2 = np.array(qt.identity(N_cav2))\n",
    "            for j in range(P_cav2, N_cav2):\n",
    "                partial_I2[j, j] = 0\n",
    "            partial_I2 = qt.Qobj(partial_I2)\n",
    "            self.P2_matrix = tfq.qt2tf(qt.tensor(qt.identity(2), partial_I2))\n",
    "\n",
    "    def _construct_optimization_masks(\n",
    "        self, beta_mask=None,gamma_mask = None, alpha1_mask=None, alpha2_mask=None, phi1_mask=None, phi2_mask=None,eta_mask=None, theta1_mask=None, theta2_mask=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        EG: What is a mask?\n",
    "        \n",
    "        Edit: Added gamma\n",
    "        \"\"\"\n",
    "        #print('construct opt masks called')\n",
    "        if beta_mask is None:\n",
    "            beta_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            if self.parameters[\"no_CD_end\"]:\n",
    "                beta_mask[-1, :] = 0  # don't optimize final CD\n",
    "        else:\n",
    "            # TODO: add mask to self.parameters for saving if it's non standard!\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if gamma_mask is None:\n",
    "            gamma_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            if self.parameters[\"no_CD_end\"]:\n",
    "                gamma_mask[-1, :] = 0  # don't optimize final CD\n",
    "        else:\n",
    "            # TODO: add mask to self.parameters for saving if it's non standard!\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if alpha1_mask is None:\n",
    "            alpha1_mask = np.ones(\n",
    "                shape=(1, self.parameters[\"N_multistart\"]), dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if alpha2_mask is None:\n",
    "            alpha2_mask = np.ones(\n",
    "                shape=(1, self.parameters[\"N_multistart\"]), dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if phi1_mask is None:\n",
    "            phi1_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            phi1_mask[0, :] = 0  # stop gradient on first phi entry\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if phi2_mask is None:\n",
    "            phi2_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            phi2_mask[0, :] = 0  # stop gradient on first phi entry\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if eta_mask is None:\n",
    "            eta_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            eta_mask[0, :] = 0  # stop gradient on first phi entry\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if theta1_mask is None:\n",
    "            theta1_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        if theta2_mask is None:\n",
    "            theta2_mask = np.ones(\n",
    "                shape=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"need to implement non-standard masks for batch optimization\"\n",
    "            )\n",
    "        self.beta_mask = beta_mask\n",
    "        self.gamma_mask = gamma_mask\n",
    "        self.alpha1_mask = alpha1_mask\n",
    "        self.alpha2_mask = alpha2_mask\n",
    "        self.phi1_mask = phi1_mask\n",
    "        self.phi2_mask = phi2_mask\n",
    "        self.eta_mask = eta_mask\n",
    "        self.theta1_mask = theta1_mask\n",
    "        self.theta2_mask = theta2_mask\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_construct_displacement_operators_mode1(self, alphas1):\n",
    "\n",
    "        # Reshape amplitudes for broadcast against diagonals\n",
    "        sqrt2 = tf.math.sqrt(tf.constant(2, dtype=tf.complex64))\n",
    "        re_a1 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.real(alphas1), dtype=tf.complex64),\n",
    "            [alphas1.shape[0], alphas1.shape[1], 1],\n",
    "        )\n",
    "        im_a1 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.imag(alphas1), dtype=tf.complex64),\n",
    "            [alphas1.shape[0], alphas1.shape[1], 1],\n",
    "        )\n",
    "\n",
    "        # Exponentiate diagonal matrices\n",
    "        expm_q1 = tf.linalg.diag(tf.math.exp(1j * im_a1 * self._eig_q1))\n",
    "        expm_p1 = tf.linalg.diag(tf.math.exp(-1j * re_a1 * self._eig_p1))\n",
    "        expm_c1 = tf.linalg.diag(tf.math.exp(-0.5 * re_a1 * im_a1 * self._qp1_comm))\n",
    "\n",
    "        # Apply Baker-Campbell-Hausdorff\n",
    "        return tf.cast(\n",
    "            self._U_q1\n",
    "            @ expm_q1\n",
    "            @ tf.linalg.adjoint(self._U_q1)\n",
    "            @ self._U_p1\n",
    "            @ expm_p1\n",
    "            @ tf.linalg.adjoint(self._U_p1)\n",
    "            @ expm_c1,\n",
    "            dtype=tf.complex64,\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_construct_displacement_operators_mode2(self, alphas2):\n",
    "\n",
    "        # Reshape amplitudes for broadcast against diagonals\n",
    "        sqrt2 = tf.math.sqrt(tf.constant(2, dtype=tf.complex64))\n",
    "        re_a2 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.real(alphas2), dtype=tf.complex64),\n",
    "            [alphas2.shape[0], alphas2.shape[1], 1],\n",
    "        )\n",
    "        im_a2 = tf.reshape(\n",
    "            sqrt2 * tf.cast(tf.math.imag(alphas2), dtype=tf.complex64),\n",
    "            [alphas2.shape[0], alphas2.shape[1], 1],\n",
    "        )\n",
    "\n",
    "        # Exponentiate diagonal matrices\n",
    "        #second mode\n",
    "        expm_q2 = tf.linalg.diag(tf.math.exp(1j * im_a2 * self._eig_q2))\n",
    "        expm_p2 = tf.linalg.diag(tf.math.exp(-1j * re_a2 * self._eig_p2))\n",
    "        expm_c2 = tf.linalg.diag(tf.math.exp(-0.5 * re_a2 * im_a2 * self._qp2_comm))\n",
    "       \n",
    "    # Apply Baker-Campbell-Hausdorff\n",
    "        return tf.cast(\n",
    "            self._U_q2\n",
    "            @ expm_q2\n",
    "            @ tf.linalg.adjoint(self._U_q2)\n",
    "            @ self._U_p2\n",
    "            @ expm_p2\n",
    "            @ tf.linalg.adjoint(self._U_p2)\n",
    "            @ expm_c2,\n",
    "            dtype=tf.complex64,\n",
    "        )\n",
    "    @tf.function\n",
    "    def get_tensor_product_displacement(self, single_mode_disp, mode):\n",
    "        '''\n",
    "        Input: displacement ops for a single mode and the index of the corresponding mode\n",
    "        Output: tensor product each displacement with identity acting on the other mode\n",
    "        '''\n",
    "        ds = single_mode_disp # Shape: (N_blocks x N_multistarts x (N_oscillator x 2) x (N_oscillator x 2))\n",
    "        \n",
    "        ds_full =[] #with tensor product\n",
    "        \n",
    "        if mode == 1: \n",
    "            #other mode is 2 \n",
    "            identity = tf.cast(np.array(qt.identity(self.parameters[\"N_cav2\"])), dtype =tf.complex64)\n",
    "        elif mode == 2:\n",
    "            identity = tf.cast(np.array(qt.identity(self.parameters[\"N_cav1\"])), dtype =tf.complex64)\n",
    "        else: \n",
    "            print('Error in block matrices: Assuming only 2 modes')\n",
    "                  \n",
    "            \n",
    "        for block_tensor in ds:\n",
    "            block_list_full = []\n",
    "            \n",
    "#             print('*******************')\n",
    "#             print(block_tensor)\n",
    "#             print(block_tensor.shape)\n",
    "#             print('*******************')\n",
    "            for multi_tensor in block_tensor:\n",
    "                #multi_list_full = []\n",
    "\n",
    "                if mode == 1:\n",
    "                    d1 = multi_tensor\n",
    "                    d2 = identity\n",
    "                elif mode == 2:\n",
    "                    d1 = identity\n",
    "                    d2 = multi_tensor\n",
    "                else:\n",
    "                    print('Error in block matrices: Assuming only 2 modes')\n",
    "\n",
    "                # tensor product\n",
    "\n",
    "                operator_1 = tf.linalg.LinearOperatorFullMatrix(d1.numpy())\n",
    "                operator_2 = tf.linalg.LinearOperatorFullMatrix(d2.numpy())\n",
    "                operator = tf.linalg.LinearOperatorKronecker([operator_1, operator_2])\n",
    "                mat = tf.cast(operator.to_dense().numpy(), dtype = tf.complex64)\n",
    "\n",
    "                #multi_list_full.append(mat)\n",
    "                block_list_full.append(mat)\n",
    "\n",
    "            ds_full.append(block_list_full)\n",
    "        ds_full = tf.cast(ds_full, dtype =tf.complex64 )\n",
    "        return ds_full\n",
    "\n",
    "    @tf.function\n",
    "    def batch_construct_block_operators(\n",
    "        self, betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas, mode = 1\n",
    "    ):\n",
    "        '''\n",
    "        Mode can be either 1 (using betas=betas) or 2 (using betas=gammas)\n",
    "        '''\n",
    "        # conditional displacements\n",
    "        Bs = (\n",
    "            tf.cast(betas_rho, dtype=tf.complex64)\n",
    "            / tf.constant(2, dtype=tf.complex64)\n",
    "            * tf.math.exp(\n",
    "                tf.constant(1j, dtype=tf.complex64)\n",
    "                * tf.cast(betas_angle, dtype=tf.complex64)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # final displacement\n",
    "        D = tf.cast(alphas_rho, dtype=tf.complex64) * tf.math.exp(\n",
    "            tf.constant(1j, dtype=tf.complex64)\n",
    "            * tf.cast(alphas_angle, dtype=tf.complex64)\n",
    "        )\n",
    "        #displacement ops\n",
    "        \n",
    "        if mode == 1:\n",
    "            ds_end_partial = self.batch_construct_displacement_operators_mode1(D)\n",
    "            ds_end = self.get_tensor_product_displacement(ds_end_partial, mode)\n",
    "            ds_g_partial = self.batch_construct_displacement_operators_mode1(Bs)    \n",
    "            ds_g = self.get_tensor_product_displacement(ds_g_partial, mode)\n",
    "        \n",
    "        elif mode == 2:\n",
    "            ds_end_partial = self.batch_construct_displacement_operators_mode2(D)\n",
    "            ds_end = self.get_tensor_product_displacement(ds_end_partial, mode)\n",
    "            ds_g_partial = self.batch_construct_displacement_operators_mode2(Bs)    \n",
    "            ds_g = self.get_tensor_product_displacement(ds_g_partial, mode) \n",
    "\n",
    "        ds_e = tf.linalg.adjoint(ds_g)\n",
    "        #end displacement ops\n",
    "\n",
    "        Phis = phis - tf.constant(np.pi, dtype=tf.float32) / tf.constant(\n",
    "            2, dtype=tf.float32\n",
    "        )\n",
    "        Thetas = thetas / tf.constant(2, dtype=tf.float32)\n",
    "        Phis = tf.cast(\n",
    "            tf.reshape(Phis, [Phis.shape[0], Phis.shape[1], 1, 1]), dtype=tf.complex64\n",
    "        )\n",
    "        Thetas = tf.cast(\n",
    "            tf.reshape(Thetas, [Thetas.shape[0], Thetas.shape[1], 1, 1]),\n",
    "            dtype=tf.complex64,\n",
    "        )\n",
    "\n",
    "        exp = tf.math.exp(tf.constant(1j, dtype=tf.complex64) * Phis)\n",
    "        exp_dag = tf.linalg.adjoint(exp)\n",
    "        cos = tf.math.cos(Thetas)\n",
    "        sin = tf.math.sin(Thetas)\n",
    "\n",
    "        # constructing the blocks of the matrix\n",
    "        ul = cos * ds_g\n",
    "        ll = exp * sin * ds_e\n",
    "        ur = tf.constant(-1, dtype=tf.complex64) * exp_dag * sin * ds_g\n",
    "        lr = cos * ds_e\n",
    "\n",
    "        # without pi pulse, block matrix is:\n",
    "        # (ul, ur)\n",
    "        # (ll, lr)\n",
    "        # however, with pi pulse included:\n",
    "        # (ll, lr)\n",
    "        # (ul, ur)\n",
    "        # pi pulse also adds -i phase, however don't need to trck it unless using multiple oscillators.a\n",
    "        # append a final block matrix with a single displacement in each quadrant\n",
    "        \n",
    "        #Shape of blocks: (N_blocks x N_multistarts x (N_oscillator x 2) x (N_oscillator x 2))\n",
    "        blocks = tf.concat(\n",
    "            [\n",
    "                -1j * tf.concat([tf.concat([ll, lr], 3), tf.concat([ul, ur], 3)], 2),\n",
    "                tf.concat(\n",
    "                    [\n",
    "                        tf.concat([ds_end, tf.zeros_like(ds_end)], 3),\n",
    "                        tf.concat([tf.zeros_like(ds_end), ds_end], 3),\n",
    "                    ],\n",
    "                    2,\n",
    "                ),\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        return blocks\n",
    "\n",
    "    # batch computation of <D>\n",
    "    # todo: handle non-pure states (rho)\n",
    "    def characteristic_function(self, psi, betas):\n",
    "        psi = tfq.qt2tf(psi)\n",
    "        betas_flat = betas.flatten()\n",
    "        betas_tf = tf.constant(\n",
    "            [betas_flat]\n",
    "        )  # need to add extra dimension since it usually batches circuits\n",
    "        Ds = tf.squeeze(self.batch_construct_displacement_operators(betas_tf))\n",
    "        num_pts = betas_tf.shape[1]\n",
    "        psis = tf.constant(np.array([psi] * num_pts))\n",
    "        C = tf.linalg.adjoint(psis) @ Ds @ psis\n",
    "        return np.squeeze(C.numpy()).reshape(betas.shape)\n",
    "\n",
    "    def characteristic_function_rho(self, rho, betas):\n",
    "        rho = tfq.qt2tf(rho)\n",
    "        betas_flat = betas.flatten()\n",
    "        betas_tf = tf.constant(\n",
    "            [betas_flat]\n",
    "        )  # need to add extra dimension since it usually batches circuits\n",
    "        Ds = tf.squeeze(self.batch_construct_displacement_operators(betas_tf))\n",
    "        num_pts = betas_tf.shape[1]\n",
    "        rhos = tf.constant(np.array([rho] * num_pts))\n",
    "        C = tf.linalg.trace(Ds @ rhos)\n",
    "        return np.squeeze(C.numpy()).reshape(betas.shape)\n",
    "\n",
    "    \"\"\"\n",
    "    @tf.function\n",
    "    def state(\n",
    "        self,\n",
    "        i=0,\n",
    "        betas_rho=None,\n",
    "        betas_angle=None,\n",
    "        alphas_rho=None,\n",
    "        alphas_angle=None,\n",
    "        phis=None,\n",
    "        thetas=None,\n",
    "    ):\n",
    "        betas_rho = self.betas_rho if betas_rho is None else betas_rho\n",
    "        betas_angle = self.betas_angle if betas_angle is None else betas_angle\n",
    "        alphas_rho = self.alphas_rho if alphas_rho is None else alphas_rho\n",
    "        alphas_angle = self.alphas_angle if alphas_angle is None else alphas_angle\n",
    "        phis = self.phis if phis is None else phis\n",
    "        thetas = self.thetas if thetas is None else thetas\n",
    "        if self.parameters[\"use_displacements\"]:\n",
    "            bs = self.construct_block_operators_with_displacements(\n",
    "                betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "            )\n",
    "        else:\n",
    "            bs = self.construct_block_operators(betas_rho, betas_angle, phis, thetas)\n",
    "        psi = self.initial_states[0]\n",
    "        for U in bs[:i]:\n",
    "            psi = U @ psi\n",
    "        return psi\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def batch_state_transfer_fidelities(\n",
    "        self, betas_rho, betas_angle,gammas_rho, gammas_angle,\n",
    "        alphas1_rho, alphas1_angle, alphas2_rho, alphas2_angle,\n",
    "        phis1, phis2, etas, thetas1, thetas2\n",
    "    ):\n",
    "        bs = self.batch_construct_block_operators(\n",
    "            betas_rho, betas_angle, alphas1_rho, alphas1_angle, phis1, thetas1, mode = 1\n",
    "        )\n",
    "        gs = self.batch_construct_block_operators(\n",
    "            gammas_rho, gammas_angle, alphas2_rho, alphas2_angle, phis2, thetas2, mode = 2\n",
    "        )\n",
    "        \n",
    "        full_blocks = []\n",
    "        for block_ind in range(len(bs)):\n",
    "            # first divide into blocks\n",
    "            bs_block = bs[block_ind]\n",
    "            gs_block = gs[block_ind]\n",
    "            \n",
    "            new_block = [] #initialize for later\n",
    "            \n",
    "            #now divide into arrrays for each multistart\n",
    "            for multistart_ind in range(len(bs_block)):\n",
    "                \n",
    "                bs_arr = bs_block[multistart_ind]\n",
    "                gs_arr = gs_block[multistart_ind]\n",
    "                \n",
    "                #now we muliply the two matrices and convert it to normal array\n",
    "                new_arr =tf.linalg.matmul(bs_arr, gs_arr)\n",
    "                new_arr = (new_arr.numpy()).tolist()\n",
    "                \n",
    "                new_block.append(new_arr)\n",
    "            #after full block complete, put into full blocks\n",
    "            full_blocks.append(new_block)\n",
    "            \n",
    "        #now convert full_blocks to tensor\n",
    "        full_blocks = tf.convert_to_tensor(full_blocks, dtype = tf.complex64)\n",
    "\n",
    "        psis = tf.stack([self.initial_states] * self.parameters[\"N_multistart\"])\n",
    "        for U in full_blocks:\n",
    "            psis = tf.einsum(\n",
    "                \"mij,msjk->msik\", U, psis\n",
    "            )  # m: multistart, s:multiple states\n",
    "        overlaps = self.target_states_dag @ psis  # broadcasting\n",
    "        overlaps = tf.reduce_mean(overlaps, axis=1)\n",
    "        overlaps = tf.squeeze(overlaps)\n",
    "        # squeeze after reduce_mean which uses axis=1,\n",
    "        # which will not exist if squeezed before for single state transfer\n",
    "        fids = tf.cast(overlaps * tf.math.conj(overlaps), dtype=tf.float32)\n",
    "        return fids\n",
    "\n",
    "    # here, including the relative phase in the cost function by taking the real part of the overlap then squaring it.\n",
    "    # need to think about how this is related to the fidelity.\n",
    "    @tf.function\n",
    "    def batch_state_transfer_fidelities_real_part(\n",
    "        self, betas_rho, betas_angle,gammas_rho, gammas_angle,\n",
    "        alphas1_rho, alphas1_angle, alphas2_rho, alphas2_angle,\n",
    "        phis1, phis2, etas, thetas1, thetas2\n",
    "    ):\n",
    "        bs = self.batch_construct_block_operators(\n",
    "            betas_rho, betas_angle, alphas1_rho, alphas1_angle, phis1, thetas1, mode = 1\n",
    "        )\n",
    "        gs = self.batch_construct_block_operators(\n",
    "            gammas_rho, gammas_angle, alphas2_rho, alphas2_angle, phis2, thetas2, mode = 2\n",
    "        )\n",
    "        \n",
    "        full_blocks = []\n",
    "        for block_ind in range(len(bs)):\n",
    "            # first divide into blocks\n",
    "            bs_block = bs[block_ind]\n",
    "            gs_block = gs[block_ind]\n",
    "            \n",
    "            new_block = [] #initialize for later\n",
    "            \n",
    "            #now divide into arrrays for each multistart\n",
    "            for multistart_ind in range(len(bs_block)):\n",
    "                \n",
    "                bs_arr = bs_block[multistart_ind]\n",
    "                gs_arr = gs_block[multistart_ind]\n",
    "                \n",
    "                #now we muliply the two matrices and convert it to normal array\n",
    "                new_arr =tf.linalg.matmul(bs_arr, gs_arr)\n",
    "                new_arr = (new_arr.numpy()).tolist()\n",
    "                \n",
    "                new_block.append(new_arr)\n",
    "            #after full block complete, put into full blocks\n",
    "            full_blocks.append(new_block)\n",
    "            \n",
    "        #now convert full_blocks to tensor\n",
    "        full_blocks = tf.convert_to_tensor(full_blocks, dtype = tf.complex64)\n",
    "\n",
    "        \n",
    "        psis = tf.stack([self.initial_states] * self.parameters[\"N_multistart\"])\n",
    "        for U in full_blocks:\n",
    "            psis = tf.einsum(\n",
    "                \"mij,msjk->msik\", U, psis\n",
    "            )  # m: multistart, s:multiple states\n",
    "        overlaps = self.target_states_dag @ psis  # broadcasting\n",
    "        overlaps = tf.reduce_mean(tf.math.real(overlaps), axis=1)\n",
    "        overlaps = tf.squeeze(overlaps)\n",
    "        # squeeze after reduce_mean which uses axis=1,\n",
    "        # which will not exist if squeezed before for single state transfer\n",
    "        # don't need to take the conjugate anymore\n",
    "        fids = tf.cast(overlaps * overlaps, dtype=tf.float32)\n",
    "        return fids\n",
    "\n",
    "#     @tf.function\n",
    "#     def mult_bin_tf(self, a):\n",
    "#         print('mult bin tf called')\n",
    "#         while a.shape[0] > 1:\n",
    "#             if a.shape[0] % 2 == 1:\n",
    "#                 a = tf.concat(\n",
    "#                     [a[:-2], [tf.matmul(a[-2], a[-1])]], 0\n",
    "#                 )  # maybe there's a faster way to deal with immutable constants\n",
    "#             a = tf.matmul(a[::2, ...], a[1::2, ...])\n",
    "#         return a[0]\n",
    "\n",
    "#     @tf.function\n",
    "#     def U_tot(self,):\n",
    "#         print('Utot called')\n",
    "#         bs = self.batch_construct_block_operators(\n",
    "#             self.betas_rho,\n",
    "#             self.betas_angle,\n",
    "#             self.alphas_rho,\n",
    "#             self.alphas_angle,\n",
    "#             self.phis,\n",
    "#             self.etas,\n",
    "#             self.thetas,\n",
    "#         )\n",
    "#         # U_c = tf.scan(lambda a, b: tf.matmul(b, a), bs)[-1]\n",
    "#         U_c = self.mult_bin_tf(\n",
    "#             tf.reverse(bs, axis=[0])\n",
    "#         )  # [U_1,U_2,..] -> [U_N,U_{N-1},..]-> U_N @ U_{N-1} @ .. @ U_1\n",
    "#         # U_c = self.I\n",
    "#         # for U in bs:\n",
    "#         #     U_c = U @ U_c\n",
    "#         return U_c\n",
    "\n",
    "    \"\"\"\n",
    "    @tf.function\n",
    "    def unitary_fidelity(\n",
    "        self, betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "    ):\n",
    "        U_circuit = self.U_tot(\n",
    "            betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "        )\n",
    "        D = tf.constant(self.parameters[\"P_cav\"] * 2, dtype=tf.complex64)\n",
    "        overlap = tf.linalg.trace(\n",
    "            tf.linalg.adjoint(self.target_unitary) @ self.P_matrix @ U_circuit\n",
    "        )\n",
    "        return tf.cast(\n",
    "            (1.0 / D) ** 2 * overlap * tf.math.conj(overlap), dtype=tf.float32\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"EG: Main function, most of the body is in catching er\"\"\" \n",
    "    def optimize(self, do_prints=True):\n",
    "        #print('optimize called')\n",
    "        timestamp = datetime.datetime.now().strftime(TIMESTAMP_FORMAT)\n",
    "        self.timestamps.append(timestamp)\n",
    "        print(\"Start time: \" + timestamp)\n",
    "        # start time\n",
    "        start_time = time.time()\n",
    "        optimizer = tf.optimizers.Ftrl(self.parameters[\"learning_rate\"])\n",
    "        if self.parameters[\"use_displacements\"] and self.parameters[\"use_etas\"]:\n",
    "            variables = [\n",
    "                self.betas_rho,\n",
    "                self.betas_angle,\n",
    "                self.gammas_rho,\n",
    "                self.gammas_angle,\n",
    "                self.alphas1_rho,\n",
    "                self.alphas1_angle,\n",
    "                self.alphas2_rho,\n",
    "                self.alphas2_angle,\n",
    "                self.phis1,\n",
    "                self.phis2,\n",
    "                self.etas,\n",
    "                self.thetas1,\n",
    "                self.thetas2,\n",
    "            ]\n",
    "        elif self.parameters[\"use_etas\"]:\n",
    "            variables = [\n",
    "                self.betas_rho,\n",
    "                self.betas_angle,\n",
    "                self.gammas_rho,\n",
    "                self.gammas_angle,\n",
    "                self.phis1,\n",
    "                self.phis2,\n",
    "                self.etas,\n",
    "                self.thetas1,\n",
    "                self.thetas2,\n",
    "            ]\n",
    "        elif self.parameters[\"use_displacements\"]:\n",
    "            variables = [\n",
    "                self.betas_rho,\n",
    "                self.betas_angle,\n",
    "                self.gammas_rho,\n",
    "                self.gammas_angle,\n",
    "                self.alphas1_rho,\n",
    "                self.alphas1_angle,\n",
    "                self.alphas2_rho,\n",
    "                self.alphas2_angle,\n",
    "                self.phis1,\n",
    "                self.phis2,\n",
    "                self.thetas1,\n",
    "                self.thetas2,\n",
    "            ]\n",
    "        else:\n",
    "            variables = [\n",
    "                self.betas_rho,\n",
    "                self.betas_angle,\n",
    "                self.gammas_rho,\n",
    "                self.gammas_angle,\n",
    "                self.phis1,\n",
    "                self.phis2,\n",
    "                self.thetas1,\n",
    "                self.thetas2,\n",
    "            ]\n",
    "\n",
    "        @tf.function\n",
    "        def entry_stop_gradients(target, mask):\n",
    "            #print('entry stop grad called')\n",
    "            mask_h = tf.abs(mask - 1)\n",
    "            return tf.stop_gradient(mask_h * target) + mask * target\n",
    "\n",
    "        \"\"\"\n",
    "        if self.optimize_expectation:\n",
    "            @tf.function\n",
    "            def loss_fun(\n",
    "                betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "            ):\n",
    "                expect = self.expectation_value(\n",
    "                    betas_rho,\n",
    "                    betas_angle,\n",
    "                    alphas_rho,\n",
    "                    alphas_angle,\n",
    "                    phis,\n",
    "                    thetas,\n",
    "                    self.O,\n",
    "                )\n",
    "                return tf.math.log(1 - tf.math.real(expect))\n",
    "        if self.unitary_optimization:\n",
    "            if self.unitary_optimization == \"states\":\n",
    "                @tf.function\n",
    "                def loss_fun(\n",
    "                    betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "                ):\n",
    "                    fid = self.unitary_fidelity_state_decomp(\n",
    "                        betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "                    )\n",
    "                    return tf.math.log(1 - fid)\n",
    "            else:\n",
    "                @tf.function\n",
    "                def loss_fun(\n",
    "                    betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "                ):\n",
    "                    fid = self.unitary_fidelity(\n",
    "                        betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "                    )\n",
    "                    return tf.math.log(1 - fid)\n",
    "        else:\n",
    "            @tf.function\n",
    "            def loss_fun(\n",
    "                betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "            ):\n",
    "                fid = self.state_fidelity(\n",
    "                    betas_rho, betas_angle, alphas_rho, alphas_angle, phis, thetas\n",
    "                )e\n",
    "                return tf.math.log(1 - fid)\n",
    "        \"\"\"\n",
    "\n",
    "        @tf.function\n",
    "        def loss_fun(fids):\n",
    "            #print('loss fun called')\n",
    "            # I think it's important that the log is taken before the avg\n",
    "            losses = tf.math.log(1 - fids)\n",
    "            avg_loss = tf.reduce_sum(losses) / self.parameters[\"N_multistart\"]\n",
    "            return avg_loss\n",
    "\n",
    "        def callback_fun(obj, fids, dfids, epoch):\n",
    "            #print('callback fun called')\n",
    "            elapsed_time_s = time.time() - start_time\n",
    "            time_per_epoch = elapsed_time_s / epoch if epoch != 0 else 0.0\n",
    "            epochs_left = self.parameters[\"epochs\"] - epoch\n",
    "            expected_time_remaining = epochs_left * time_per_epoch\n",
    "            fidelities_np = np.squeeze(np.array(fids))\n",
    "            betas_np, gammas_np, alphas1_np, alphas2_np, phis1_np,phis2_np, etas_np, thetas1_np, thetas2_np = self.get_numpy_vars()\n",
    "            if epoch == 0:\n",
    "                self._save_optimization_data(\n",
    "                    timestamp,\n",
    "                    fidelities_np,\n",
    "                    betas_np,\n",
    "                    gammas_np,\n",
    "                    alphas1_np,\n",
    "                    alphas2_np,\n",
    "                    phis1_np,\n",
    "                    phis2_np,\n",
    "                    etas_np,\n",
    "                    thetas1_np,\n",
    "                    thetas2_np,\n",
    "                    elapsed_time_s,\n",
    "                    append=False,\n",
    "                )\n",
    "            else:\n",
    "                self._save_optimization_data(\n",
    "                    timestamp,\n",
    "                    fidelities_np,\n",
    "                    betas_np,\n",
    "                    gammas_np,\n",
    "                    alphas1_np,\n",
    "                    alphas2_np,\n",
    "                    phis1_np,\n",
    "                    phis2_np,\n",
    "                    etas_np,\n",
    "                    thetas1_np,\n",
    "                    thetas2_np,\n",
    "                    elapsed_time_s,\n",
    "                    append=True,\n",
    "                )\n",
    "            avg_fid = tf.reduce_sum(fids) / self.parameters[\"N_multistart\"]\n",
    "            max_fid = tf.reduce_max(fids)\n",
    "            avg_dfid = tf.reduce_sum(dfids) / self.parameters[\"N_multistart\"]\n",
    "            max_dfid = tf.reduce_max(dfids)\n",
    "            extra_string = \" (real part)\" if self.parameters[\"use_phase\"] else \"\"\n",
    "            if do_prints:\n",
    "                print(\n",
    "                    \"\\r Epoch: %d / %d Max Fid: %.6f Avg Fid: %.6f Max dFid: %.6f Avg dFid: %.6f\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        self.parameters[\"epochs\"],\n",
    "                        max_fid,\n",
    "                        avg_fid,\n",
    "                        max_dfid,\n",
    "                        avg_dfid,\n",
    "                    )\n",
    "                    + \" Elapsed time: \"\n",
    "                    + str(datetime.timedelta(seconds=elapsed_time_s))\n",
    "                    + \" Remaing time: \"\n",
    "                    + str(datetime.timedelta(seconds=expected_time_remaining))\n",
    "                    + extra_string,\n",
    "                    end=\"\",\n",
    "                )\n",
    "\n",
    "        initial_fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis1,\n",
    "            self.phis2,\n",
    "            self.etas,\n",
    "            self.thetas1,\n",
    "            self.thetas2,\n",
    "        )\n",
    "        fids = initial_fids\n",
    "        callback_fun(self, fids, 0, 0)\n",
    "        try:  # will catch keyboard inturrupt\n",
    "            for epoch in range(self.parameters[\"epochs\"] + 1)[1:]:\n",
    "                for _ in range(self.parameters[\"epoch_size\"]):\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        betas_rho = entry_stop_gradients(self.betas_rho, self.beta_mask)\n",
    "                        betas_angle = entry_stop_gradients(\n",
    "                            self.betas_angle, self.beta_mask\n",
    "                        )\n",
    "                        gammas_rho = entry_stop_gradients(self.gammas_rho, self.gamma_mask)\n",
    "                        gammas_angle = entry_stop_gradients(\n",
    "                            self.gammas_angle, self.gamma_mask\n",
    "                        )\n",
    "                        if self.parameters[\"use_displacements\"]:\n",
    "                            alphas1_rho = entry_stop_gradients(\n",
    "                                self.alphas1_rho, self.alpha1_mask\n",
    "                            )\n",
    "                            alphas1_angle = entry_stop_gradients(\n",
    "                                self.alphas1_angle, self.alpha1_mask\n",
    "                            )\n",
    "                            alphas2_rho = entry_stop_gradients(\n",
    "                                self.alphas2_rho, self.alpha2_mask\n",
    "                            )\n",
    "                            alphas2_angle = entry_stop_gradients(\n",
    "                                self.alphas2_angle, self.alpha2_mask\n",
    "                            )\n",
    "                        else:\n",
    "                            alphas1_rho = self.alphas1_rho\n",
    "                            alphas1_angle = self.alphas1_angle\n",
    "                            alphas2_rho = self.alphas2_rho\n",
    "                            alphas2_angle = self.alphas2_angle\n",
    "                        phis1 = entry_stop_gradients(self.phis1, self.phi1_mask)\n",
    "                        phis2 = entry_stop_gradients(self.phis2, self.phi2_mask)\n",
    "                        if self.parameters[\"use_etas\"]:\n",
    "                            etas = entry_stop_gradients(self.etas, self.eta_mask)\n",
    "                        else:\n",
    "                            etas = self.etas\n",
    "                        thetas1 = entry_stop_gradients(self.thetas1, self.theta1_mask)\n",
    "                        thetas2 = entry_stop_gradients(self.thetas2, self.theta2_mask)\n",
    "                        new_fids = self.batch_fidelities(\n",
    "                            betas_rho,\n",
    "                            betas_angle,\n",
    "                            gammas_rho,\n",
    "                            gammas_angle,\n",
    "                            alphas1_rho,\n",
    "                            alphas1_angle,\n",
    "                            alphas2_rho,\n",
    "                            alphas2_angle,\n",
    "                            phis1,\n",
    "                            phis2,\n",
    "                            etas,\n",
    "                            thetas1,\n",
    "                            thetas2,\n",
    "                        )\n",
    "                        new_loss = loss_fun(new_fids)\n",
    "                        dloss_dvar = tape.gradient(new_loss, variables)\n",
    "                        print(\"******************* Debugging gradients\")\n",
    "                        print('variables')\n",
    "                        print(variables)\n",
    "                        print('New Loss')\n",
    "                        print(new_loss)\n",
    "                        print('old fids')\n",
    "                        print(fids)\n",
    "                        print('new_fids')\n",
    "                        print(new_fids)\n",
    "                        print('dloss_dvar')\n",
    "                        print(dloss_dvar)        \n",
    "                        print(\"********************\")\n",
    "                    optimizer.apply_gradients(zip(dloss_dvar, variables))\n",
    "                dfids = new_fids - fids\n",
    "                fids = new_fids\n",
    "                callback_fun(self, fids, dfids, epoch)\n",
    "                condition_fid = tf.greater(fids, self.parameters[\"term_fid\"])\n",
    "                condition_dfid = tf.greater(dfids, self.parameters[\"dfid_stop\"])\n",
    "                if tf.reduce_any(condition_fid):\n",
    "                    print(\"\\n\\n Optimization stopped. Term fidelity reached.\\n\")\n",
    "                    termination_reason = \"term_fid\"\n",
    "                    break\n",
    "                if not tf.reduce_any(condition_dfid):\n",
    "                    print(\"\\n max dFid: %6f\" % tf.reduce_max(dfids).numpy())\n",
    "                    print(\"dFid stop: %6f\" % self.parameters[\"dfid_stop\"])\n",
    "                    print(\n",
    "                        \"\\n\\n Optimization stopped.  No dfid is greater than dfid_stop\\n\"\n",
    "                    )\n",
    "                    termination_reason = \"dfid\"\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n max dFid: %6f\" % tf.reduce_max(dfids).numpy())\n",
    "            print(\"dFid stop: %6f\" % self.parameters[\"dfid_stop\"])\n",
    "            print(\"\\n\\n Optimization stopped on keyboard interrupt\")\n",
    "            termination_reason = \"keyboard_interrupt\"\n",
    "\n",
    "        if epoch == self.parameters[\"epochs\"]:\n",
    "            termination_reason = \"epochs\"\n",
    "            print(\n",
    "                \"\\n\\nOptimization stopped.  Reached maximum number of epochs. Terminal fidelity not reached.\\n\"\n",
    "            )\n",
    "        self._save_termination_reason(timestamp, termination_reason)\n",
    "        timestamp_end = datetime.datetime.now().strftime(TIMESTAMP_FORMAT)\n",
    "        elapsed_time_s = time.time() - start_time\n",
    "        epoch_time_s = elapsed_time_s / epoch\n",
    "        step_time_s = epoch_time_s / self.parameters[\"epochs\"]\n",
    "        self.print_info()\n",
    "        print(\"all data saved as: \" + self.filename)\n",
    "        print(\"termination reason: \" + termination_reason)\n",
    "        print(\"optimization timestamp (start time): \" + timestamp)\n",
    "        print(\"timestamp (end time): \" + timestamp_end)\n",
    "        print(\"elapsed time: \" + str(datetime.timedelta(seconds=elapsed_time_s)))\n",
    "        print(\n",
    "            \"Time per epoch (epoch size = %d): \" % self.parameters[\"epoch_size\"]\n",
    "            + str(datetime.timedelta(seconds=epoch_time_s))\n",
    "        )\n",
    "        print(\n",
    "            \"Time per Adam step (N_multistart = %d, N_cav1 = %d, N_cav2 = %d): \"\n",
    "            % (self.parameters[\"N_multistart\"], self.parameters[\"N_cav1\"], self.parameters[\"N_cav2\"])\n",
    "            + str(datetime.timedelta(seconds=step_time_s))\n",
    "        )\n",
    "        print(END_OPT_STRING)\n",
    "        return timestamp\n",
    "\n",
    "    # if append is True, it will assume the dataset is already created and append only the\n",
    "    # last aquired values to it.\n",
    "    # TODO: if needed, could use compression when saving data.\n",
    "    def _save_optimization_data(\n",
    "        self,\n",
    "        timestamp,\n",
    "        fidelities_np,\n",
    "        betas_np,\n",
    "        gammas_np,\n",
    "        alphas1_np,\n",
    "        alphas2_np,\n",
    "        phis1_np,\n",
    "        phis2_np,\n",
    "        etas_np,\n",
    "        thetas1_np,\n",
    "        thetas2_np,\n",
    "        elapsed_time_s,\n",
    "        append,\n",
    "    ):\n",
    "        if not append:\n",
    "            with h5py.File(self.filename, \"a\") as f:\n",
    "                grp = f.create_group(timestamp)\n",
    "                for parameter, value in self.parameters.items():\n",
    "                    grp.attrs[parameter] = value\n",
    "                grp.attrs[\"termination_reason\"] = \"outside termination\"\n",
    "                grp.attrs[\"elapsed_time_s\"] = elapsed_time_s\n",
    "                if self.target_unitary is not None:\n",
    "                    grp.create_dataset(\n",
    "                        \"target_unitary\", data=self.target_unitary.numpy()\n",
    "                    )\n",
    "                grp.create_dataset(\"initial_states\", data=self.initial_states.numpy())\n",
    "                grp.create_dataset(\"target_states\", data=self.target_states.numpy())\n",
    "                # dims = [[2, int(self.initial_states[0].numpy().shape[0] / 2)], [1, 1]]\n",
    "                grp.create_dataset(\n",
    "                    \"fidelities\",\n",
    "                    chunks=True,\n",
    "                    data=[fidelities_np],\n",
    "                    maxshape=(None, self.parameters[\"N_multistart\"]),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"betas\",\n",
    "                    data=[betas_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"gammas\",\n",
    "                    data=[gammas_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"alphas1\",\n",
    "                    data=[alphas1_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(None, self.parameters[\"N_multistart\"], 1,),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"alphas2\",\n",
    "                    data=[alphas2_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(None, self.parameters[\"N_multistart\"], 1,),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"phis1\",\n",
    "                    data=[phis1_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"phis2\",\n",
    "                    data=[phis2_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"etas\",\n",
    "                    data=[etas_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"thetas1\",\n",
    "                    data=[thetas1_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "                grp.create_dataset(\n",
    "                    \"thetas2\",\n",
    "                    data=[thetas2_np],\n",
    "                    chunks=True,\n",
    "                    maxshape=(\n",
    "                        None,\n",
    "                        self.parameters[\"N_multistart\"],\n",
    "                        self.parameters[\"N_blocks\"],\n",
    "                    ),\n",
    "                )\n",
    "        else:  # just append the data\n",
    "            with h5py.File(self.filename, \"a\") as f:\n",
    "                f[timestamp][\"fidelities\"].resize(\n",
    "                    f[timestamp][\"fidelities\"].shape[0] + 1, axis=0\n",
    "                )\n",
    "                f[timestamp][\"betas\"].resize(f[timestamp][\"betas\"].shape[0] + 1, axis=0)\n",
    "                f[timestamp][\"gammas\"].resize(f[timestamp][\"gammas\"].shape[0] + 1, axis=0)\n",
    "                f[timestamp][\"alphas1\"].resize(\n",
    "                    f[timestamp][\"alphas1\"].shape[0] + 1, axis=0\n",
    "                )\n",
    "                f[timestamp][\"alphas2\"].resize(\n",
    "                    f[timestamp][\"alphas2\"].shape[0] + 1, axis=0\n",
    "                )\n",
    "                f[timestamp][\"phis1\"].resize(f[timestamp][\"phis1\"].shape[0] + 1, axis=0)\n",
    "                f[timestamp][\"phis2\"].resize(f[timestamp][\"phis2\"].shape[0] + 1, axis=0)\n",
    "                f[timestamp][\"etas\"].resize(f[timestamp][\"etas\"].shape[0] + 1, axis=0)\n",
    "                f[timestamp][\"thetas1\"].resize(\n",
    "                    f[timestamp][\"thetas1\"].shape[0] + 1, axis=0\n",
    "                )\n",
    "                f[timestamp][\"thetas2\"].resize(\n",
    "                    f[timestamp][\"thetas2\"].shape[0] + 1, axis=0\n",
    "                )\n",
    "\n",
    "                f[timestamp][\"fidelities\"][-1] = fidelities_np\n",
    "                f[timestamp][\"betas\"][-1] = betas_np\n",
    "                f[timestamp][\"gammas\"][-1] = gammas_np\n",
    "                f[timestamp][\"alphas1\"][-1] = alphas1_np\n",
    "                f[timestamp][\"alphas2\"][-1] = alphas2_np\n",
    "                f[timestamp][\"phis1\"][-1] = phis1_np\n",
    "                f[timestamp][\"phis2\"][-1] = phis2_np\n",
    "                f[timestamp][\"etas\"][-1] = etas_np\n",
    "                f[timestamp][\"thetas1\"][-1] = thetas1_np\n",
    "                f[timestamp][\"thetas2\"][-1] = thetas2_np\n",
    "                f[timestamp].attrs[\"elapsed_time_s\"] = elapsed_time_s\n",
    "\n",
    "    def _save_termination_reason(self, timestamp, termination_reason):\n",
    "        with h5py.File(self.filename, \"a\") as f:\n",
    "            f[timestamp].attrs[\"termination_reason\"] = termination_reason\n",
    "\n",
    "    def randomize_and_set_vars(self):\n",
    "        beta_scale = self.parameters[\"beta_scale\"]\n",
    "        gamma_scale = self.parameters[\"gamma_scale\"]\n",
    "        alpha1_scale = self.parameters[\"alpha1_scale\"]\n",
    "        alpha2_scale = self.parameters[\"alpha2_scale\"]\n",
    "        theta_scale = self.parameters[\"theta_scale\"]\n",
    "        betas_rho = np.random.uniform(\n",
    "            0,\n",
    "            beta_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        betas_angle = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        gammas_rho = np.random.uniform(\n",
    "            0,\n",
    "            gamma_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        gammas_angle = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        if self.parameters[\"use_displacements\"]:\n",
    "            alphas1_rho = np.random.uniform(\n",
    "                0, alpha1_scale, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "            alphas1_angle = np.random.uniform(\n",
    "                -np.pi, np.pi, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "            alphas2_rho = np.random.uniform(\n",
    "                0, alpha2_scale, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "            alphas2_angle = np.random.uniform(\n",
    "                -np.pi, np.pi, size=(1, self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "        phis1 = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        phis2 = np.random.uniform(\n",
    "            -np.pi,\n",
    "            np.pi,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        if self.parameters[\"use_etas\"]:  # eta range is 0 to pi.\n",
    "            etas = np.random.uniform(\n",
    "                -np.pi,\n",
    "                np.pi,\n",
    "                size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "            )\n",
    "        thetas1 = np.random.uniform(\n",
    "            -1 * theta_scale,\n",
    "            theta_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        thetas2 = np.random.uniform(\n",
    "            -1 * theta_scale,\n",
    "            theta_scale,\n",
    "            size=(self.parameters[\"N_blocks\"], self.parameters[\"N_multistart\"]),\n",
    "        )\n",
    "        phis1[0] = 0  # everything is relative to first phi\n",
    "        phis2[0] = 0  # everything is relative to first phi\n",
    "        if self.parameters[\"no_CD_end\"]:\n",
    "            betas_rho[-1] = 0\n",
    "            betas_angle[-1] = 0\n",
    "            gammas_rho[-1] = 0\n",
    "            gammas_angle[-1] = 0\n",
    "        self.betas_rho = tf.Variable(\n",
    "            betas_rho, dtype=tf.float32, trainable=True, name=\"betas_rho\",\n",
    "        )\n",
    "        self.betas_angle = tf.Variable(\n",
    "            betas_angle, dtype=tf.float32, trainable=True, name=\"betas_angle\",\n",
    "        )\n",
    "        self.gammas_rho = tf.Variable(\n",
    "            gammas_rho, dtype=tf.float32, trainable=True, name=\"gammas_rho\",\n",
    "        )\n",
    "        self.gammas_angle = tf.Variable(\n",
    "            gammas_angle, dtype=tf.float32, trainable=True, name=\"gammas_angle\",\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if self.parameters[\"use_displacements\"]:\n",
    "            self.alphas1_rho = tf.Variable(\n",
    "                alphas1_rho, dtype=tf.float32, trainable=True, name=\"alphas1_rho\",\n",
    "            )\n",
    "            self.alphas1_angle = tf.Variable(\n",
    "                alphas1_angle, dtype=tf.float32, trainable=True, name=\"alphas1_angle\",\n",
    "            )\n",
    "            self.alphas2_rho = tf.Variable(\n",
    "                alphas2_rho, dtype=tf.float32, trainable=True, name=\"alphas2_rho\",\n",
    "            )\n",
    "            self.alphas2_angle = tf.Variable(\n",
    "                alphas2_angle, dtype=tf.float32, trainable=True, name=\"alphas2_angle\",\n",
    "            )\n",
    "        else:\n",
    "            self.alphas1_rho = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            self.alphas1_angle = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            self.alphas2_rho = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            self.alphas2_angle = tf.constant(\n",
    "                np.zeros(shape=((1, self.parameters[\"N_multistart\"]))),\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "        self.phis1 = tf.Variable(phis1, dtype=tf.float32, trainable=True, name=\"phis1\",)\n",
    "        self.phis2 = tf.Variable(phis2, dtype=tf.float32, trainable=True, name=\"phis2\",)\n",
    "        if self.parameters[\"use_etas\"]:\n",
    "            self.etas = tf.Variable(\n",
    "                etas, dtype=tf.float32, trainable=True, name=\"etas\",\n",
    "            )\n",
    "        else:\n",
    "            self.etas = tf.constant(\n",
    "                (np.pi / 2.0) * np.ones_like(phis1), dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        self.thetas1 = tf.Variable(\n",
    "            thetas1, dtype=tf.float32, trainable=True, name=\"thetas1\",\n",
    "        )\n",
    "        self.thetas2 = tf.Variable(\n",
    "            thetas2, dtype=tf.float32, trainable=True, name=\"thetas2\",\n",
    "        )\n",
    "\n",
    "    def get_numpy_vars(\n",
    "        self,\n",
    "        betas_rho=None,\n",
    "        betas_angle=None,\n",
    "        gammas_rho=None,\n",
    "        gammas_angle=None,\n",
    "        alphas1_rho=None,\n",
    "        alphas1_angle=None,\n",
    "        alphas2_rho=None,\n",
    "        alphas2_angle=None,\n",
    "        phis1=None,\n",
    "        phis2=None,\n",
    "        etas=None,\n",
    "        thetas1=None,\n",
    "        thetas2=None,\n",
    "    ):\n",
    "        betas_rho = self.betas_rho if betas_rho is None else betas_rho\n",
    "        betas_angle = self.betas_angle if betas_angle is None else betas_angle\n",
    "        gammas_rho = self.gammas_rho if gammas_rho is None else gammas_rho\n",
    "        gammas_angle = self.gammas_angle if gammas_angle is None else gammas_angle\n",
    "        alphas1_rho = self.alphas1_rho if alphas1_rho is None else alphas1_rho\n",
    "        alphas1_angle = self.alphas1_angle if alphas1_angle is None else alphas1_angle\n",
    "        alphas2_rho = self.alphas2_rho if alphas2_rho is None else alphas2_rho\n",
    "        alphas2_angle = self.alphas2_angle if alphas2_angle is None else alphas2_angle\n",
    "        phis1 = self.phis1 if phis1 is None else phis1\n",
    "        phis2 = self.phis2 if phis2 is None else phis2\n",
    "        etas = self.etas if etas is None else etas\n",
    "        thetas1 = self.thetas1 if thetas1 is None else thetas1\n",
    "        thetas2 = self.thetas2 if thetas2 is None else thetas2\n",
    "\n",
    "        betas = betas_rho.numpy() * np.exp(1j * betas_angle.numpy())\n",
    "        gammas = gammas_rho.numpy() * np.exp(1j * gammas_angle.numpy())\n",
    "        alphas1 = alphas1_rho.numpy() * np.exp(1j * alphas1_angle.numpy())\n",
    "        alphas2 = alphas2_rho.numpy() * np.exp(1j * alphas2_angle.numpy())\n",
    "        phis1 = phis1.numpy()\n",
    "        phis2 = phis2.numpy()\n",
    "        etas = etas.numpy()\n",
    "        thetas1 = thetas1.numpy()\n",
    "        thetas2 = thetas2.numpy()\n",
    "        # now, to wrap phis, etas, and thetas so it's in the range [-pi, pi]\n",
    "        phis1 = (phis1 + np.pi) % (2 * np.pi) - np.pi\n",
    "        phis2 = (phis2 + np.pi) % (2 * np.pi) - np.pi\n",
    "        etas = (etas + np.pi) % (2 * np.pi) - np.pi\n",
    "        thetas1 = (thetas1 + np.pi) % (2 * np.pi) - np.pi\n",
    "        thetas2 = (thetas2 + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "        # these will have shape N_multistart x N_blocks\n",
    "        return betas.T, gammas.T, alphas1.T, alphas2.T, phis1.T, phis2.T, etas.T, thetas1.T, thetas2.T\n",
    "\n",
    "    def set_tf_vars(self, betas=None, gammas=None, alphas1=None, alphas2=None, phis1=None, phis2=None,etas=None, thetas1=None, thetas2=None):\n",
    "        # reshaping for N_multistart = 1\n",
    "        if betas is not None:\n",
    "            if len(betas.shape) < 2:\n",
    "                betas = betas.reshape(betas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            betas_rho = np.abs(betas)\n",
    "            betas_angle = np.angle(betas)\n",
    "            self.betas_rho = tf.Variable(\n",
    "                betas_rho, dtype=tf.float32, trainable=True, name=\"betas_rho\"\n",
    "            )\n",
    "            self.betas_angle = tf.Variable(\n",
    "                betas_angle, dtype=tf.float32, trainable=True, name=\"betas_angle\",\n",
    "            )\n",
    "        if gammas is not None:\n",
    "            if len(gammas.shape) < 2:\n",
    "                gammas = gammas.reshape(betas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            gammas_rho = np.abs(gammas)\n",
    "            gammas_angle = np.angle(gammas)\n",
    "            self.gammas_rho = tf.Variable(\n",
    "                gammas_rho, dtype=tf.float32, trainable=True, name=\"gammas_rho\"\n",
    "            )\n",
    "            self.gammas_angle = tf.Variable(\n",
    "                gammas_angle, dtype=tf.float32, trainable=True, name=\"gammas_angle\",\n",
    "            )\n",
    "        if alphas1 is not None:\n",
    "            if len(alphas1.shape) < 2:\n",
    "                alphas1 = alphas1.reshape(alphas1.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            alphas1_rho = np.abs(alphas1)\n",
    "            alphas1_angle = np.angle(alphas1)\n",
    "         \n",
    "            if self.parameters[\"use_displacements\"]:\n",
    "                self.alphas1_rho = tf.Variable(\n",
    "                    alphas1_rho, dtype=tf.float32, trainable=True, name=\"alphas1_rho\",\n",
    "                )\n",
    "                self.alphas1_angle = tf.Variable(\n",
    "                    alphas1_angle, dtype=tf.float32, trainable=True, name=\"alphas1_angle\",\n",
    "                )\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                self.alphas1_rho = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                self.alphas1_angle = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                \n",
    "        if alphas2 is not None:\n",
    "            if len(alphas2.shape) < 2:\n",
    "                alphas2 = alphas2.reshape(alphas2.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            alphas2_rho = np.abs(alphas2)\n",
    "            alphas2_angle = np.angle(alphas2)\n",
    "            if self.parameters[\"use_displacements\"]:                \n",
    "                self.alphas2_rho = tf.Variable(\n",
    "                    alphas2_rho, dtype=tf.float32, trainable=True, name=\"alphas2_rho\",\n",
    "                )\n",
    "                self.alphas2_angle = tf.Variable(\n",
    "                    alphas2_angle, dtype=tf.float32, trainable=True, name=\"alphas2_angle\",\n",
    "                )\n",
    "            else:\n",
    "                self.alphas2_rho = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                self.alphas2_angle = tf.constant(\n",
    "                    np.zeros(shape=((1, self.parameters[\"N_multistart\"],))),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "\n",
    "\n",
    "        if phis1 is not None:\n",
    "            if len(phis1.shape) < 2:\n",
    "                phis1 = phis.reshape(phis1.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.phis1 = tf.Variable(\n",
    "                phis1, dtype=tf.float32, trainable=True, name=\"phis1\",\n",
    "            )\n",
    "        if phis2 is not None:\n",
    "            if len(phis2.shape) < 2:\n",
    "                phis2 = phis2.reshape(phis.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.phis2 = tf.Variable(\n",
    "                phis2, dtype=tf.float32, trainable=True, name=\"phis2\",\n",
    "            )\n",
    "        if etas is not None:\n",
    "            if len(etas.shape) < 2:\n",
    "                etas = etas.reshape(etas.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.etas = tf.Variable(\n",
    "                etas, dtype=tf.float32, trainable=True, name=\"etas\",\n",
    "            )\n",
    "        if thetas1 is not None:\n",
    "            if len(thetas1.shape) < 2:\n",
    "                thetas1 = thetas1.reshape(thetas1.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.thetas1 = tf.Variable(\n",
    "                thetas1, dtype=tf.float32, trainable=True, name=\"thetas1\",\n",
    "            )\n",
    "        if thetas2 is not None:\n",
    "            if len(thetas2.shape) < 2:\n",
    "                thetas2 = thetas2.reshape(thetas2.shape + (1,))\n",
    "                self.parameters[\"N_multistart\"] = 1\n",
    "            self.thetas2 = tf.Variable(\n",
    "                thetas2, dtype=tf.float32, trainable=True, name=\"thetas2\",\n",
    "            )\n",
    "\n",
    "    def best_circuit(self):\n",
    "        fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis1,\n",
    "            self.phis2,\n",
    "            self.etas,\n",
    "            self.thetas1,\n",
    "            self.thetas2,\n",
    "        )\n",
    "        fids = np.atleast_1d(fids.numpy())\n",
    "        max_idx = np.argmax(fids)\n",
    "        all_betas, all_gammas,all_alphas1, all_alphas2, all_phis1, all_phis2, all_etas, all_thetas1, all_thetas2 = self.get_numpy_vars(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis1,\n",
    "            self.phis2,\n",
    "            self.etas,\n",
    "            self.thetas1,\n",
    "            self.thetas2,\n",
    "        )\n",
    "        max_fid = fids[max_idx]\n",
    "        betas = all_betas[max_idx]\n",
    "        gammas = all_gammas[max_idx]\n",
    "        alphas1 = all_alphas1[max_idx]\n",
    "        alphas2 = all_alphas2[max_idx]\n",
    "        phis1 = all_phis1[max_idx]\n",
    "        phis2 = all_phis2[max_idx]\n",
    "        etas = all_etas[max_idx]\n",
    "        thetas1 = all_thetas1[max_idx]\n",
    "        thetas2 = all_thetas2[max_idx]\n",
    "        return {\n",
    "            \"fidelity\": max_fid,\n",
    "            \"betas\": betas,\n",
    "            \"gammas\": gammas,\n",
    "            \"alphas1\": alphas1,\n",
    "            \"alphas2\": alphas2,\n",
    "            \"phis1\": phis1,\n",
    "            \"phis2\": phis2,\n",
    "            \"etas\": etas,\n",
    "            \"thetas1\": thetas1,\n",
    "            \"thetas2\": thetas2,\n",
    "        }\n",
    "\n",
    "    def all_fidelities(self):\n",
    "        fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis1,\n",
    "            self.phis2,\n",
    "            self.etas,\n",
    "            self.thetas1,\n",
    "            self.thetas2,\n",
    "        )\n",
    "        return fids.numpy()\n",
    "\n",
    "    def best_fidelity(self):\n",
    "        fids = self.batch_fidelities(\n",
    "            self.betas_rho,\n",
    "            self.betas_angle,\n",
    "            self.gammas_rho,\n",
    "            self.gammas_angle,\n",
    "            self.alphas1_rho,\n",
    "            self.alphas1_angle,\n",
    "            self.alphas2_rho,\n",
    "            self.alphas2_angle,\n",
    "            self.phis1,\n",
    "            self.phis2,\n",
    "            self.etas,\n",
    "            self.thetas1,\n",
    "            self.thetas2,\n",
    "        )\n",
    "        max_idx = tf.argmax(fids).numpy()\n",
    "        max_fid = fids[max_idx].numpy()\n",
    "        return max_fid\n",
    "\n",
    "    def print_info(self):\n",
    "        best_circuit = self.best_circuit()\n",
    "        with np.printoptions(precision=5, suppress=True):\n",
    "            for parameter, value in self.parameters.items():\n",
    "                print(parameter + \": \" + str(value))\n",
    "            print(\"filename: \" + self.filename)\n",
    "            print(\"\\nBest circuit parameters found:\")\n",
    "            print(\"betas:         \" + str(best_circuit[\"betas\"]))\n",
    "            print(\"gammas:         \" + str(best_circuit[\"gammas\"]))\n",
    "            print(\"alphas1:        \" + str(best_circuit[\"alphas1\"]))\n",
    "            print(\"alphas2:        \" + str(best_circuit[\"alphas2\"]))\n",
    "            print(\"phis1 (deg):    \" + str(best_circuit[\"phis1\"] * 180.0 / np.pi))\n",
    "            print(\"phis2 (deg):    \" + str(best_circuit[\"phis2\"] * 180.0 / np.pi))\n",
    "            print(\"etas (deg):    \" + str(best_circuit[\"etas\"] * 180.0 / np.pi))\n",
    "            print(\"thetas1 (deg):  \" + str(best_circuit[\"thetas1\"] * 180.0 / np.pi))\n",
    "            print(\"thetas2 (deg):  \" + str(best_circuit[\"thetas2\"] * 180.0 / np.pi))\n",
    "            print(\"Max Fidelity:  %.6f\" % best_circuit[\"fidelity\"])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c41f8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21b1364c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[10, 10], [1, 1]], shape = (100, 1), type = ket\\begin{equation*}\\left(\\begin{array}{*{11}c}1.0\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\\\vdots\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[10, 10], [1, 1]], shape = (100, 1), type = ket\n",
       "Qobj data =\n",
       "[[1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The initial oscillator state.\n",
    "N1 =10\n",
    "N2 =10\n",
    "Fock1 = 0\n",
    "Fock2= 0\n",
    "psi_i1 = qt.basis(N1,Fock1) #target state\n",
    "psi_i2 = qt.basis(N2,Fock2)\n",
    "psi_initial = qt.tensor(psi_i1, psi_i2)\n",
    "psi_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7322db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[10, 10], [1, 1]], shape = (100, 1), type = ket\\begin{equation*}\\left(\\begin{array}{*{11}c}0.0\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\\\vdots\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[10, 10], [1, 1]], shape = (100, 1), type = ket\n",
       "Qobj data =\n",
       "[[0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The target oscillator state.\n",
    "Fock1 = 1\n",
    "Fock2= 0\n",
    "psi_t1 = qt.basis(N1,Fock1) #target state\n",
    "psi_t2 = qt.basis(N2,Fock2)\n",
    "psi_target = qt.tensor(psi_t1, psi_t2)\n",
    "psi_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1302cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization of ECD Circuit parameters (betas, phis, and thetas)\n",
    "#the optimization options\n",
    "opt_params = {\n",
    "'N_blocks' : 5, #circuit depth\n",
    "'N_multistart' : 3, #Batch size (number of circuit optimizations to run in parallel)\n",
    "'epochs' : 4, #number of epochs before termination\n",
    "'epoch_size' : 20, #number of adam steps per epoch\n",
    "'learning_rate' : 0.01, #adam learning rate\n",
    "'term_fid' : 0.995, #terminal fidelitiy\n",
    "'dfid_stop' : 1e-6, #stop if dfid between two epochs is smaller than this number\n",
    "'beta_scale' : 3.0, #maximum |beta| for random initialization\n",
    "'gamma_scale' : 3.0, #maximum |gamma| for random initialization\n",
    "'N_cav1': N1, #number of levels in mode 1\n",
    "'N_cav2': N2, #number of levels in mode 2\n",
    "'initial_states' : [qt.tensor(qt.basis(2,0),psi_initial)], #qubit tensor oscillator, start in |g> |0>\n",
    "'target_states' : [qt.tensor(qt.basis(2,0), psi_target)], #end in |e> |target>.\n",
    "'name' : 'Fock1 %d' % Fock1, #name for printing and saving\n",
    "'filename' : None, #if no filename specified, results will be saved in this folder under 'name.h5'\n",
    "}\n",
    "\n",
    "\n",
    "#note: optimizer includes pi pulse in every ECD step. However, final ECD step is implemented \n",
    "#in experiment as a displacement since the qubit and oscillator should be disentangled at this point.\n",
    "#So, we ask the optimizer to end in |e> |target> instead of |g>|target>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b99b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "175a7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization_type: state transfer\n",
      "N_multistart: 3\n",
      "N_blocks: 5\n",
      "term_fid: 0.995\n",
      "dfid_stop: 1e-06\n",
      "no_CD_end: False\n",
      "learning_rate: 0.01\n",
      "epoch_size: 20\n",
      "epochs: 4\n",
      "beta_scale: 3.0\n",
      "gamma_scale: 3.0\n",
      "alpha1_scale: 1.0\n",
      "alpha2_scale: 1.0\n",
      "theta_scale: 3.141592653589793\n",
      "use_etas: False\n",
      "use_displacements: False\n",
      "use_phase: False\n",
      "name: Fock1 1\n",
      "comment: \n",
      "N_cav1: 10\n",
      "N_cav2: 10\n",
      "filename: Fock1 1.h5\n",
      "\n",
      "Best circuit parameters found:\n",
      "betas:         [ 1.2123 -2.31428j -0.86259+0.05356j  1.73942-0.89242j -2.06707+1.61195j\n",
      "  0.2528 -1.36028j]\n",
      "gammas:         [ 1.43984+0.04863j  1.41717+0.60454j -0.96298+0.29489j -0.00235-0.03067j\n",
      " -0.01089+0.02874j]\n",
      "alphas1:        [0.+0.j]\n",
      "alphas2:        [0.+0.j]\n",
      "phis1 (deg):    [   0.        72.12157 -124.34177 -173.13103  -68.26923]\n",
      "phis2 (deg):    [  0.      -66.86303 -87.64565  63.02687 179.73969]\n",
      "etas (deg):    [89.99999 89.99999 89.99999 89.99999 89.99999]\n",
      "thetas1 (deg):  [  66.89999  -13.87449   79.61977 -105.54488   97.27332]\n",
      "thetas2 (deg):  [-177.44249   19.69783  -78.9356   -64.45558  100.21165]\n",
      "Max Fidelity:  0.098321\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create optimization object. \n",
    "#initial params will be randomized upon creation\n",
    "opt = BatchOptimizer(**opt_params)\n",
    "\n",
    "#print optimization info. \n",
    "opt.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "052c9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2022-05-15 05:54:12\n",
      " Epoch: 0 / 4 Max Fid: 0.098321 Avg Fid: 0.035622 Max dFid: 0.000000 Avg dFid: 0.000000 Elapsed time: 0:00:00.555369 Remaing time: 0:00:00******************* Debugging gradients\n",
      "variables\n",
      "[<tf.Variable 'betas_rho:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[1.4048578 , 1.3241693 , 2.6125813 ],\n",
      "       [2.4514587 , 2.541205  , 0.86425346],\n",
      "       [1.606529  , 2.7810912 , 1.9549924 ],\n",
      "       [1.0788879 , 0.7173581 , 2.6212864 ],\n",
      "       [1.5866755 , 2.3176665 , 1.3835738 ]], dtype=float32)>, <tf.Variable 'betas_angle:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-1.2812164 ,  0.86167544, -1.0882621 ],\n",
      "       [ 1.6754953 , -2.9260387 ,  3.0795815 ],\n",
      "       [-1.7330413 ,  0.7538624 , -0.4740357 ],\n",
      "       [-1.151411  , -1.7583907 ,  2.4792762 ],\n",
      "       [-1.2549026 ,  0.8459699 , -1.3870497 ]], dtype=float32)>, <tf.Variable 'gammas_rho:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[1.9180418 , 2.4293196 , 1.4406655 ],\n",
      "       [0.3020275 , 0.7040932 , 1.5407249 ],\n",
      "       [0.4857597 , 1.2680911 , 1.0071232 ],\n",
      "       [2.6591606 , 2.3456306 , 0.03075941],\n",
      "       [0.55765516, 1.8846753 , 0.03073544]], dtype=float32)>, <tf.Variable 'gammas_angle:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-3.0179625 ,  1.808212  ,  0.03376497],\n",
      "       [-1.2811685 ,  1.3946935 ,  0.40320882],\n",
      "       [ 0.5479877 , -1.0467653 ,  2.8444326 ],\n",
      "       [ 0.43597034,  2.3339884 , -1.6472691 ],\n",
      "       [-3.130919  , -2.0168705 ,  1.9328587 ]], dtype=float32)>, <tf.Variable 'phis1:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [-2.9140015 ,  1.4114809 ,  1.2587588 ],\n",
      "       [-0.05125755,  0.17479299, -2.1701734 ],\n",
      "       [-0.9752723 ,  0.5679133 , -3.0217066 ],\n",
      "       [-3.0656466 , -2.4941266 , -1.1915228 ]], dtype=float32)>, <tf.Variable 'phis2:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [-0.14006476, -2.1679418 , -1.16698   ],\n",
      "       [ 2.01258   , -3.1085355 , -1.5297052 ],\n",
      "       [-2.9057841 ,  0.333807  ,  1.1000261 ],\n",
      "       [-2.576209  , -1.0964385 ,  3.1370494 ]], dtype=float32)>, <tf.Variable 'thetas1:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-2.551909  , -2.093719  ,  1.1676251 ],\n",
      "       [-1.4767205 ,  1.6220336 , -0.24215564],\n",
      "       [ 2.7557273 ,  1.0219893 ,  1.3896273 ],\n",
      "       [-1.0502409 ,  0.5006851 , -1.8421059 ],\n",
      "       [-3.1236491 , -0.04320322,  1.6977401 ]], dtype=float32)>, <tf.Variable 'thetas2:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 1.1548113 ,  1.5416558 , -3.0969555 ],\n",
      "       [-1.7426941 ,  3.0553186 ,  0.34379193],\n",
      "       [-1.2278585 , -1.1324077 , -1.3776863 ],\n",
      "       [ 1.3875659 , -0.7550991 , -1.1249622 ],\n",
      "       [-1.826556  , -2.210048  ,  1.7490231 ]], dtype=float32)>]\n",
      "New Loss\n",
      "tf.Tensor(-0.037356373, shape=(), dtype=float32)\n",
      "old fids\n",
      "tf.Tensor([0.00112953 0.00741417 0.09832147], shape=(3,), dtype=float32)\n",
      "new_fids\n",
      "tf.Tensor([0.00112953 0.00741417 0.09832147], shape=(3,), dtype=float32)\n",
      "dloss_dvar\n",
      "[None, None, None, None, None, None, None, None]\n",
      "********************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['betas_rho:0', 'betas_angle:0', 'gammas_rho:0', 'gammas_angle:0', 'phis1:0', 'phis2:0', 'thetas1:0', 'thetas2:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'betas_rho:0' shape=(5, 3) dtype=float32, numpy=\narray([[1.4048578 , 1.3241693 , 2.6125813 ],\n       [2.4514587 , 2.541205  , 0.86425346],\n       [1.606529  , 2.7810912 , 1.9549924 ],\n       [1.0788879 , 0.7173581 , 2.6212864 ],\n       [1.5866755 , 2.3176665 , 1.3835738 ]], dtype=float32)>), (None, <tf.Variable 'betas_angle:0' shape=(5, 3) dtype=float32, numpy=\narray([[-1.2812164 ,  0.86167544, -1.0882621 ],\n       [ 1.6754953 , -2.9260387 ,  3.0795815 ],\n       [-1.7330413 ,  0.7538624 , -0.4740357 ],\n       [-1.151411  , -1.7583907 ,  2.4792762 ],\n       [-1.2549026 ,  0.8459699 , -1.3870497 ]], dtype=float32)>), (None, <tf.Variable 'gammas_rho:0' shape=(5, 3) dtype=float32, numpy=\narray([[1.9180418 , 2.4293196 , 1.4406655 ],\n       [0.3020275 , 0.7040932 , 1.5407249 ],\n       [0.4857597 , 1.2680911 , 1.0071232 ],\n       [2.6591606 , 2.3456306 , 0.03075941],\n       [0.55765516, 1.8846753 , 0.03073544]], dtype=float32)>), (None, <tf.Variable 'gammas_angle:0' shape=(5, 3) dtype=float32, numpy=\narray([[-3.0179625 ,  1.808212  ,  0.03376497],\n       [-1.2811685 ,  1.3946935 ,  0.40320882],\n       [ 0.5479877 , -1.0467653 ,  2.8444326 ],\n       [ 0.43597034,  2.3339884 , -1.6472691 ],\n       [-3.130919  , -2.0168705 ,  1.9328587 ]], dtype=float32)>), (None, <tf.Variable 'phis1:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  0.        ],\n       [-2.9140015 ,  1.4114809 ,  1.2587588 ],\n       [-0.05125755,  0.17479299, -2.1701734 ],\n       [-0.9752723 ,  0.5679133 , -3.0217066 ],\n       [-3.0656466 , -2.4941266 , -1.1915228 ]], dtype=float32)>), (None, <tf.Variable 'phis2:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  0.        ],\n       [-0.14006476, -2.1679418 , -1.16698   ],\n       [ 2.01258   , -3.1085355 , -1.5297052 ],\n       [-2.9057841 ,  0.333807  ,  1.1000261 ],\n       [-2.576209  , -1.0964385 ,  3.1370494 ]], dtype=float32)>), (None, <tf.Variable 'thetas1:0' shape=(5, 3) dtype=float32, numpy=\narray([[-2.551909  , -2.093719  ,  1.1676251 ],\n       [-1.4767205 ,  1.6220336 , -0.24215564],\n       [ 2.7557273 ,  1.0219893 ,  1.3896273 ],\n       [-1.0502409 ,  0.5006851 , -1.8421059 ],\n       [-3.1236491 , -0.04320322,  1.6977401 ]], dtype=float32)>), (None, <tf.Variable 'thetas2:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 1.1548113 ,  1.5416558 , -3.0969555 ],\n       [-1.7426941 ,  3.0553186 ,  0.34379193],\n       [-1.2278585 , -1.1324077 , -1.3776863 ],\n       [ 1.3875659 , -0.7550991 , -1.1249622 ],\n       [-1.826556  , -2.210048  ,  1.7490231 ]], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0df2e72ed479>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run optimizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-a9848ea7fd3e>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, do_prints)\u001b[0m\n\u001b[0;32m   1039\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdloss_dvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"********************\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdloss_dvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mdfids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_fids\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m                 \u001b[0mfids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_fids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     \"\"\"\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\u001b[0m\u001b[0;32m     74\u001b[0m                      f\"Provided `grads_and_vars` is {grads_and_vars}.\")\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['betas_rho:0', 'betas_angle:0', 'gammas_rho:0', 'gammas_angle:0', 'phis1:0', 'phis2:0', 'thetas1:0', 'thetas2:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'betas_rho:0' shape=(5, 3) dtype=float32, numpy=\narray([[1.4048578 , 1.3241693 , 2.6125813 ],\n       [2.4514587 , 2.541205  , 0.86425346],\n       [1.606529  , 2.7810912 , 1.9549924 ],\n       [1.0788879 , 0.7173581 , 2.6212864 ],\n       [1.5866755 , 2.3176665 , 1.3835738 ]], dtype=float32)>), (None, <tf.Variable 'betas_angle:0' shape=(5, 3) dtype=float32, numpy=\narray([[-1.2812164 ,  0.86167544, -1.0882621 ],\n       [ 1.6754953 , -2.9260387 ,  3.0795815 ],\n       [-1.7330413 ,  0.7538624 , -0.4740357 ],\n       [-1.151411  , -1.7583907 ,  2.4792762 ],\n       [-1.2549026 ,  0.8459699 , -1.3870497 ]], dtype=float32)>), (None, <tf.Variable 'gammas_rho:0' shape=(5, 3) dtype=float32, numpy=\narray([[1.9180418 , 2.4293196 , 1.4406655 ],\n       [0.3020275 , 0.7040932 , 1.5407249 ],\n       [0.4857597 , 1.2680911 , 1.0071232 ],\n       [2.6591606 , 2.3456306 , 0.03075941],\n       [0.55765516, 1.8846753 , 0.03073544]], dtype=float32)>), (None, <tf.Variable 'gammas_angle:0' shape=(5, 3) dtype=float32, numpy=\narray([[-3.0179625 ,  1.808212  ,  0.03376497],\n       [-1.2811685 ,  1.3946935 ,  0.40320882],\n       [ 0.5479877 , -1.0467653 ,  2.8444326 ],\n       [ 0.43597034,  2.3339884 , -1.6472691 ],\n       [-3.130919  , -2.0168705 ,  1.9328587 ]], dtype=float32)>), (None, <tf.Variable 'phis1:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  0.        ],\n       [-2.9140015 ,  1.4114809 ,  1.2587588 ],\n       [-0.05125755,  0.17479299, -2.1701734 ],\n       [-0.9752723 ,  0.5679133 , -3.0217066 ],\n       [-3.0656466 , -2.4941266 , -1.1915228 ]], dtype=float32)>), (None, <tf.Variable 'phis2:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 0.        ,  0.        ,  0.        ],\n       [-0.14006476, -2.1679418 , -1.16698   ],\n       [ 2.01258   , -3.1085355 , -1.5297052 ],\n       [-2.9057841 ,  0.333807  ,  1.1000261 ],\n       [-2.576209  , -1.0964385 ,  3.1370494 ]], dtype=float32)>), (None, <tf.Variable 'thetas1:0' shape=(5, 3) dtype=float32, numpy=\narray([[-2.551909  , -2.093719  ,  1.1676251 ],\n       [-1.4767205 ,  1.6220336 , -0.24215564],\n       [ 2.7557273 ,  1.0219893 ,  1.3896273 ],\n       [-1.0502409 ,  0.5006851 , -1.8421059 ],\n       [-3.1236491 , -0.04320322,  1.6977401 ]], dtype=float32)>), (None, <tf.Variable 'thetas2:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 1.1548113 ,  1.5416558 , -3.0969555 ],\n       [-1.7426941 ,  3.0553186 ,  0.34379193],\n       [-1.2278585 , -1.1324077 , -1.3776863 ],\n       [ 1.3875659 , -0.7550991 , -1.1249622 ],\n       [-1.826556  , -2.210048  ,  1.7490231 ]], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "#run optimizer.\n",
    "opt.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d0bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
